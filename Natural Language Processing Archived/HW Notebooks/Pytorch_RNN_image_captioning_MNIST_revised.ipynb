{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Revisions\n","\n","Revision: Changed img embedding concatination from seq_len_dim(dim=1 at t=0) to embedding_len_dim(dim=2)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1626,"status":"ok","timestamp":1716545455773,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"2OoUWgrFi82h"},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716545461949,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"MN1Z7uGbxTdq","scrolled":true},"outputs":[],"source":["# %pip install torch torchvision pillow spacy numpy\n","# %pip install torchtext\n","# %pip install pycocotools"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716545461950,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"5EDyhvMIxybA"},"outputs":[],"source":["import os\n","import math\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from torchvision.datasets import MNIST\n","\n","from tqdm import tqdm\n","\n","from PIL import Image\n","import spacy"]},{"cell_type":"markdown","metadata":{"id":"pzeLAtu9zQFv"},"source":["## Building the tokenizer and vocabulary"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716545461950,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"I2kg8R_nhN1A"},"outputs":[],"source":["spacy_eng = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716545461950,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"y-3CxVtzxcd6"},"outputs":[],"source":["def word_tokenize(text):\n","    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716545461950,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"mFOLfFKebpCf"},"outputs":[],"source":["numerical_to_word_numbers = {\n","    0: \"Zero\",\n","    1: \"One\",\n","    2: \"Two\",\n","    3: \"Three\",\n","    4: \"Four\",\n","    5: \"Five\",\n","    6: \"Six\",\n","    7: \"Seven\",\n","    8: \"Eight\",\n","    9: \"Nine\"\n","}\n","\n","possible_caption_templates = [\n","    \"This is an image of the number <number>\",\n","    \"We can see the number <number> in this image\",\n","    \"Number <number> is displayed here\",\n","    \"In this image, the number <number> is shown\",\n","    \"Here, you can see the number <number>\",\n","    \"The image features the number <number>\",\n","    \"Displayed in the image is the number <number>\"\n","]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1716545461950,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"_JQ8Ow_t7B7P","outputId":"3d426c95-3f30-4ef2-e308-7555acfae567"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [00:00<00:00, 8487.46it/s]\n"]}],"source":["# Define the vocabulary and tokenizer\n","word_to_index = {'<PAD>':0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n","\n","for text in tqdm(possible_caption_templates+list(numerical_to_word_numbers.values())):\n","    tokens = word_tokenize(text.lower())\n","    for token in tokens:\n","        if token not in word_to_index:\n","            idx = len(word_to_index)\n","            word_to_index[token] = idx\n","\n","index_to_word = {it: k for k, it in word_to_index.items()}"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716545461950,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Mbv23I3-xUee","outputId":"b407a877-60dd-42e0-a8c4-3343ff617721"},"outputs":[{"data":{"text/plain":["['<', 'sos', '>', 'testing', 'tokenization', 'breakdown', '<', 'eos', '>']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["word_tokenize('<SOS> Testing tokenization breakdown <EOS>')  # We will manually add tokens for <EOS> and <SOS> etc after tokenization to avoid them breaking up."]},{"cell_type":"markdown","metadata":{"id":"xKBTTh7IrvTK"},"source":["## Defining the dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1716545463624,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"0hs0N5Ouay0H"},"outputs":[],"source":["def get_random_caption(num):\n","\n","    template_idx = random.randint(0, len(possible_caption_templates)-1)\n","    template = possible_caption_templates[template_idx]\n","\n","    caption = template.replace('<number>', numerical_to_word_numbers[num])\n","\n","    return caption\n","\n","\n","def convert_sentence_to_idxs(sentence):\n","\n","    words = word_tokenize(sentence)\n","    idxs = [word_to_index[word] for word in words]\n","\n","    return idxs\n","\n","\n","def convert_idxs_to_sentence(idxs):\n","\n","    words = [index_to_word[idx] for idx in idxs]\n","    return ' '.join(words)\n","\n","\n","class MNISTCaptionsDataset(Dataset):\n","\n","    def __init__(self, *args, **kwargs):\n","\n","        self.mnist = MNIST(*args, **kwargs)\n","        self.max_seq_len = 50\n","\n","    def __len__(self):\n","        return len(self.mnist)\n","\n","    def __getitem__(self, idx):\n","\n","        img, label = self.mnist[idx]\n","        caption = get_random_caption(label)\n","\n","        caption_idxs = convert_sentence_to_idxs(caption.lower().strip())\n","        if len(caption_idxs) > self.max_seq_len - 2:  # 2 for SOS and EOS\n","            caption_idxs = caption_idxs[:self.max_seq_len-2]\n","\n","        padding_len = self.max_seq_len - len(caption_idxs) - 2  # need to pad to make it to seq_len. All captions Need to be of same length so they can be stacked\n","\n","        caption_numeric = (\n","            [word_to_index['<SOS>']]+\n","            caption_idxs+\n","            [word_to_index['<EOS>']]+\n","            [word_to_index['<PAD>']]*padding_len\n","        )\n","\n","        caption_numeric = torch.Tensor(caption_numeric)\n","\n","        return img, caption_numeric, caption\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i79_gIk-1zut"},"source":["## Defining the Model"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":823,"status":"ok","timestamp":1716545762575,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"D59I4JSZe46D"},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(EncoderCNN, self).__init__()\n","\n","        # Normally, it would be a good idea to use a pre-trained model as a backbone for feature extraction\n","        # However, most pretrained models expect RGB images and ones that are bigger in size than 28x28\n","        # For this reason, you have been provided with commented code to see how we could have used\n","        # resnet backbone in case of some other dataset but we will use our own custom CNN\n","        # model for feature extraction.\n","        # We could have converted MNIST images to RGB and upscaled their size but that would have\n","        # drastically increased the training time.\n","\n","        # self.resnet = models.resnet50(pretrained=True).requires_grad_(False)  # resnet embedding backbone\n","        # self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n","        # self.relu = nn.ReLU()\n","        # self.times = []\n","\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","        self.fc2 = nn.Linear(128, embed_size)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = x.view(-1, 64 * 7 * 7)\n","        x = self.relu(self.fc1(x))\n","        embedding = self.fc2(x)\n","        embedding = self.relu(embedding)\n","\n","        return embedding\n","\n","    # def forward(self, images):\n","\n","    #     features = self.resnet(images)\n","    #     return self.relu(features)\n","\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","\n","        self.dropout = nn.Dropout(0.5)\n","        self.rnn = nn.LSTM(2*embed_size, hidden_size, num_layers, batch_first=True)  # 2* because we'll concat image embeddings with every token's embedding\n","\n","        # NOTE: We can replace the LSTM architecture by RNN or GRU by replacing the above line by one of the following\n","\n","        # self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n","        # self.rnn = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n","\n","        self.linear = nn.Sequential(\n","            nn.Linear(hidden_size, 1024),\n","            nn.Linear(1024, vocab_size)\n","        )\n","\n","    def forward(self, features, captions):\n","\n","        # embeddings = self.embed(captions)\n","        # embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=-2)\n","        # hiddens, _ = self.rnn(embeddings)\n","        # outputs = self.linear(hiddens)\n","        # return outputs\n","    \n","        embeddings = self.dropout(self.embed(captions))\n","        seq_length = embeddings.shape[-2]\n","\n","        repeat_shape = [1]*embeddings.ndim\n","        repeat_shape[-2] = seq_length\n","        features = features.unsqueeze(-2).repeat(repeat_shape)  # add sequence length dimension. Make it the same shape as caption embeddings.\n","\n","        combined_embeddings = torch.cat((embeddings, features), dim=-1)\n","\n","        outputs, _ = self.rnn(combined_embeddings)\n","        outputs = self.linear(outputs)\n","        return outputs\n","\n","\n","class ImageCaptioner(nn.Module):\n","\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(ImageCaptioner, self).__init__()\n","        self.encoder = EncoderCNN(embed_size)\n","        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","    def forward(self, images, captions):\n","        features = self.encoder(images)\n","        outputs = self.decoder(features, captions)\n","        return outputs\n","\n","    def caption_image(self, image, max_length=50):\n","\n","        result_caption = []\n","\n","        with torch.no_grad():\n","\n","            img_embedding = self.encoder(image)\n","            x = self.decoder.embed(torch.tensor([word_to_index['<SOS>']]).to(img_embedding.device))\n","\n","            states = None\n","\n","            for _ in range(max_length):\n","\n","                x = torch.cat((x, img_embedding), dim=-1)\n","\n","                output, states = self.decoder.rnn(x, states)\n","                output = self.decoder.linear(output.squeeze(0))\n","\n","                predicted = output.argmax(0)\n","                pred_token = index_to_word[predicted.item()]\n","                result_caption.append(pred_token)\n","                x = self.decoder.embed(predicted).unsqueeze(0)\n","\n","                if pred_token == \"<EOS>\":\n","                    break\n","\n","        return result_caption"]},{"cell_type":"markdown","metadata":{},"source":["## Initializing the model and dataset"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1716545763360,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"3OVeS6Txbbzc"},"outputs":[],"source":["train_transforms = transforms.Compose(\n","        [\n","            # If we want to upscale MNIST images and convert to RGB\n","            # transforms.Grayscale(num_output_channels=3),\n","            # transforms.Resize((299, 299)),\n","\n","            transforms.ToTensor(),\n","        ]\n","    )\n","\n","batch_size = 128\n","total_dataset = MNISTCaptionsDataset(root='./datasets', train=False, download=True, transform=train_transforms)\n","\n","train_dataset = Subset(total_dataset, range(len(total_dataset) - 100))\n","test_dataset = Subset(total_dataset, range(len(total_dataset) - 100, len(total_dataset)))\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","clear_output()"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716545763361,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"rlsyakwKrQ2Z","outputId":"eeb1b8fb-a584-48dd-b5b1-48bafa863981"},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":829,"status":"ok","timestamp":1716545773337,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"5YB-RYSH2DmL"},"outputs":[],"source":["embed_size = 128\n","hidden_size = 128\n","vocab_size = len(word_to_index)\n","num_decoder_layers = 1\n","learning_rate = 5e-5\n","num_epochs = 70"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716545774472,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"v7-7f4gmhr5u"},"outputs":[],"source":["# initialize model, loss etc\n","model = ImageCaptioner(embed_size, hidden_size, vocab_size, num_decoder_layers).to(device)\n","criterion = nn.CrossEntropyLoss(ignore_index=word_to_index['<PAD>'])  # ignore pad token loss calculations\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"Db6Iq3hU3GMk"},"source":["## Pre-training Testing"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716545775163,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"QtS8hnpH3K3D"},"outputs":[],"source":["test_samples = [test_dataset[i] for i in range(10, 13)]\n","test_imgs, test_caption_tensors, test_captions = list(zip(*test_samples))\n","\n","test_imgs = torch.stack(test_imgs, 0).to(device)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":45},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1716545776984,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Ptk58laBofd_","outputId":"79ed5fb3-5f70-4e16-f06c-e12c603b0155"},"outputs":[{"data":{"image/jpeg":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABUABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+lRGd1RFLMxwFAySa1E8Oar/a9lplzZT2dzeOiRC5hdM7iADjGSOewNQa1pNzoWtXmlXZjNxaStFIY23KSPQ+lUa9j+D/AIfh0/RNZ8a6mfsS2sW2wvpYzKqZysjiMcsRwAemWNUNQ+JmmaMFPhuO/wBT1VEZBrWtyGWSIt1MMZJCZ9fzBry6aWS4mkmmdpJZGLu7HJYk5JJrT8MaK3iLxNp+kLIIxczBGkIyEXqzfgATXq1tqMupWfjXUY4TZ+EItHfTdMV8qjlT+6255ZidzH/eOa8ToqezvbrTruO7srma2uYzlJoJCjrxjhhyODVnUNd1jV4ootS1W+vY4c+WlzcPIEz1wGJx0FZ9FFeteB/hjo/9gr4p8d6gun6UwEltAZgrToOpOPmweMBfmP5Z6fw94v0jxte6h4StPCdlD4Ut7OWYOsX72PYuFk6gbiTx3569a+fq7z4Y+GLLVrzUte1hDJo2gQC7uoVwWmPzFUweoOxs/THeqHiXxFr3xN8VxuttJNM37qzsoFyI0znAH8yf5Curu7jTvhl4G1HQYLuG68W6soivXtyCLKI9Yt/QnqCB3PsM+T113gbx0/g19Sgm02HU9N1OEQ3dpI5TeBnGGAOPvMOnetbVvizdSWr2fhnQ9N8N2zqVZ7OMGcg5yPMwOOewB9688JLMWYkknJJ70lFFddoXgaTUfC+oeJNUv10rSrdCIJpIi5upu0aDIzyCCRnHp1xyNFdp8P8AwI/iu8kvtQl+xeHbHL31652gADO1SeN3T6A59AbPxH8fr4nmh0jR4/svhrT8JaW6rt34GN7D8TgenvmuCrqvAfhCPxdrE8d3qEen6bZQm5vLlyPkjBA4yepz16D9DsePvHdrqNlD4W8Lwmy8M2eAqqSGumBzvf1GecHvz6Y89ooooor/2Q==","image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAABUCAAAAAC0bQhJAAACt0lEQVR4Ae2VT0hUURTGT1OTjqOGkGGNWRqlGUJhhpFTWhiS1kKmQmtji9oUUqssXGhChruIgnJh0V8DCVSkP0Y6WFogSak1M2rFJIOZmk6aNd+190bvvDvP95Ytit7mfOf7nfPevee9O0P0d1yJLzoTiQzqxRqiZcfRuilDTYgO1g092UO0zjW+Vw2NLexzm+MKrepiN9SMill/Ei2JLHGz7jg1zJlgxWSp6mbs4Qo1i5+cOpNxzsF8dYfmrZNokn30sWnnEXWXnMdOM/a6fIsWomwnmyo1i0i5ualiuTPE+10bnkizOWmlyBSdOlm/uIhdXqg4irL0DSZTwogrVLGUwa+Jt/fQmDdkgQjndFgXiyCKdg+YRDi32sKUJi/RtqgZkXFdOhxDlP+eVXJDjPZm2lDuYhc3iibX9qmWQcayg+bD2T8aYyq72S7NN0eRBb8gXXcsGlvf91JG0nV+HjzdPAr8PJWV1Yi7amgdllp6jkq2DVDB9G/y/Q7Lrg2XguGxH4DvTZz8nRjaZp+5iFcszTGymse1/jQznbuzMbwXvgvc2g9YuZZixD3AxXOrB01GnkixELgfOK63ge0Cyx3Dg7U83z2K/mU8IQp9jv7AUc704kOSwqgKnxJ4mjmNAf9euTGD9jkZVub1XQ08wG8yDhOfwXOAtxAFhkC0IzI1J63jeKcC/eodxk82dEgvcqg2VoX8u5Rnjqfi9niVoaAXeFURFcKN/5Eo5SvL1ZtDciPGtQYp11uqMbpTpzHMDuTpMNM1wKN1AOX6MomV6DRavwD5OmzrCFAU9LMqFE4A1TqvPK2eoUb45IQuMtexmeuiIWhTH6DHzNLhcgi1ogy/BTToPC/8JtC+WaxWtFnqcwdOoeLLyvoWaF0d7PHM+Aho0v7fJDorrWU9r1THPN25qCv/fP4bDzAbicjEkfIAAAAASUVORK5CYII=","text/plain":["<PIL.Image.Image image mode=L size=28x84>"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["transforms.functional.to_pil_image(test_imgs.reshape([1, 1, -1, 28])[0])"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["('The image features the number Eight',\n"," 'This is an image of the number Zero',\n"," 'This is an image of the number One')"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["test_captions"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1716545776984,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"xiAYu3QXtHki","outputId":"e758c0c5-aaf0-490b-ff5e-a7f1e3f12edb"},"outputs":[{"name":"stdout","output_type":"stream","text":["number > of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n","--------------------\n","number > of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n","--------------------\n","number > of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n"]}],"source":["# Predicted captions\n","\n","model.eval()\n","captions = []\n","\n","for img in test_imgs:\n","    with torch.no_grad():\n","        caption = model.caption_image(img.unsqueeze(0))\n","        caption = ' '.join(caption)\n","        captions.append(caption)\n","\n","print(('\\n'+'-'*20+'\\n').join(captions))"]},{"cell_type":"markdown","metadata":{"id":"oOQLLt_F3iIs"},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpeCGaogiE0Q","outputId":"4d1337ee-3a83-46fb-f75b-110d43a3d286"},"outputs":[],"source":["for epoch in range(num_epochs):\n","\n","    model.train()\n","\n","    for idx, (imgs, captions, _) in tqdm(\n","        enumerate(train_loader), total=len(train_loader), leave=False\n","    ):\n","        imgs = imgs.to(device)\n","        captions = captions.to(device).type(torch.long)\n","\n","        outputs = model(imgs, captions[:, :-1])\n","        loss = criterion(\n","            outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1)\n","        )\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    if epoch % 10 == 0:\n","        with torch.no_grad():\n","\n","            model.eval()\n","            captions = []\n","\n","            for img in test_imgs:\n","                with torch.no_grad():\n","                    caption = model.caption_image(img.unsqueeze(0))\n","                    caption = ' '.join(caption)\n","                    captions.append(caption)\n","\n","            print(('\\n'+'-'*20+'\\n').join(captions))\n","\n","    print(f'\\nEpoch: {epoch+1}/{num_epochs}', \"Training loss: \", loss.item())"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the results"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"8Tg5Dg9tMIpr"},"outputs":[{"data":{"image/jpeg":"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","image/png":"iVBORw0KGgoAAAANSUhEUgAAARgAAAAcCAAAAACfA//yAAAHt0lEQVR4Ae2Yf1CVVRrHTyIgCv6ARRfFQNRUSCUQlTAajRZ/kOMoVq6jrmZjrVuz/XJ1yl+RdNW1MS3vCmoWtHGtXHIYLC2YVRFNk0ADAjTSS17hGl5+3Btwnufued97wfuec14znZqdxvMH73m+n+c573mf8/NCyJ1yJwO3m4ENlQtvt4nfY3w3YzuO/O0+zHdKYdOzv93rbuNNyYhru99GvF6ooURGVh6m9MK/ZETV5lT10WU3AiFRI26Eb4WNasGPAn9pYMRb1ig1ZrRuZNLVMxKWC7R2RYgEuKTkltZHdeENwLBL1PGGPnc+r8dW1zgvhUphSDOe9ZYSfbH3ziZsHRIRk7bT7CjScfMqxOMiSmmzvRBESKxIXMpywCUyFj7tiawsEx67R4SJVyyvF9mAUsfnIlSVXosO6E2np84DwNOyOO90zO8vA0wLjHq5FQD3x/N8KiI2Z5zJWp+aj008dNvxgNECCq+DLEH0FIxQ7OVpK/W4A2/uaQZkXwA4j4ek73mWE3oxNfUczRegKoSUQcMgCQp5PmGZw/ZcDpgkMOYMVkVLdLKsYH98sfJKCnQp7zDqomV7jCLOaoHHeeiyAw5jmZ+A4il9ThA9hAd+gqMepqs6prDKUVlZuHY1wDYha+MPsQ5mrvojIRV0uxAakD+ekFcB1gmEkBkfA8C5BeRt2CjSmQ5sYKFCGVUBtMNObcdWBE8HOk3gbiH0Cm7SYYuhWk2dFrPE7NAqWisXMUWrqJb/GPbofhJK/iDAdQBHXNPhCqznadw3cD+Ja4AKHjD77+dZXiDPS5qY2aVY9KAkqM/3bBio9cmhjO0AGixxUaQB19Agon7xbFin1MLrIiLhZtq+WqK7pUQb6NOVAFPF0Lk5r7jEpQ4ayeEtiJ8O6H0IW+ZwgBBvgxV+GNcACYQchfk87v4q1ArLRHHaqSygbWFKdYkZNuqd5edwe6Dioylz683swDXAl0Ea2W2ktKP5XhlQte1QNlMXWrFUlzFQQD/j+jkGO071JFsAcsQ432+heXFwpjmUkH3wEs+DbPZneU21l589u9w1P8Oq4IjUhYRMym3cxlLAl/lo9CMLEHX2khNAD/vyMW57bjvorUxCHqcwSSdOkSdYaCqH1+EppuwCcxQHmLkIIIMQnwR/ElpnFkbXgLl6fXQ3NfgbSpPFZpnS7zPERBnx+nMkubsc93LD1+kaUknp5E6De1bCBX9Oum7aIKvHdYuvjbbSgt5acdxPLWz69bXDO1pdsXxq4INeqtwtu12YULHW75Qk3+W9KPku1Un4099EabGgEuKXmlPJTmxJYqaXvB9OyG4o0tuXyHxKr0maJKRnurNtiZQo4hiEYSKMevkkgBNOvGV1NvK9MagH3MfYGi3E9fwH1Ix1qSOqL8/juH8VftiNkIcWfo+4Rz5SD1N68D4ujGUyOg+x7sW38bCAkmqg7F4SW1u9QkCdQkAJQECn4fkc+QPmedqaum8JlruG2ENOLXDfJ5RLBf95pAFLJ5Cxl52GoISElVs9wghZD62dA/cfmKJBzEjCtgcJmfQjnl7yJc7nqWpbqbBwmT4dETIDiK+YmFF1zoIIdnFAk7Q5l8hmoXT/qQccoRs2Gp0v8HC2nVoup6+5J9ioJIZ9i7Z8Adhe3gDYcLHpiiHGkyWchjS3HdtW85AnUuo5eJSQcUeq/uoTeLJZoIrHcifsU55cSao7wLLsk4tLOTCwFnDyvE3ZHWDanPWkBg5O7zJZYlK6jK5KRH0HLPLqMrlKj4/gHL/DpNpppuq2xqIkJsOHi4nMd12W4XgUR/bCCT+3tLVpDgcJOchGfHi9ZQoJM4lZU7yjK6F2pBDGBB/2AX7zsJTvykCLenFHqD+2STuAsfXU6G5pYDOlYqMD2AL7RJQ7lRhgNzWuFNBMXyYNYhet4kf2A32G46RH5Fp2hUsL4rs5pALmun0NtRP78mFkFlpzM9H+2is2u0GATOj1AW0X1t+QqWlsOxo/4x1r87/5ISRk8sJ1e02ARmEPiXWA0xzH+tBz8GmAA+LrvkUsGSDKboVdest5+IAqhf+lChxpcYRchULuVGIB9wNs5uOI9wKaoU5Nn9BPLYslx07YV+xgUUruNCFYEYoo/RMPdl/Dpo3Z1YjV/2SdkZYtuFeiP2Vn+2SFyVTEbo5f9xccViEcChTULuExsE/tMtyVg3QNGbbsKqX5ak8mXKZZvAu79EqyPdjq+u0wogCyQ/kQ1fbdcvQ7bMxIlkIyy0a/Foid5bHZbN78GD89uzyjLNWd67dLUyox7P6i/M6gsE+YTyT0K2wcr3HXGMGlkksv0C/YPxtaL0zs7vLd33F+uiaK/RwCiOAkZvYrh2z2eM8CrwWL9GeVkEtQKY7hE8b0xKE3jM2Dh+W8b+iLm09RapwpyVs+4BvyKFXdCpa7BbyL0rI3Uyde19+lq64bSi0J4bhkAFliPp+U10avzpEsI20LMms3bZ0t039Oy1Kum7+whNlgQ5h+DLvbfShS38hI7aYSHKnsxR5lBzbx55FKU66xPRmKx3q43nw1EdgS/v8off6LZYNuoStJjchNIXcj/mllYBwumbg38ZKoRjBx+b+JqF/H5RnADb9Oy7fQ6t+odfgthN0J+T1m4H8avnH5ROLqagAAAABJRU5ErkJggg==","text/plain":["<PIL.Image.Image image mode=L size=280x28>"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["random_test_img_idxs = random.sample(list(range(len(test_dataset))), 10)\n","random_test_imgs = [test_dataset[i][0] for i in random_test_img_idxs]\n","\n","transforms.functional.to_pil_image(torch.cat(random_test_imgs, -1))"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["['number six is displayed here <EOS>',\n"," 'in this image , the number seven is shown <EOS>',\n"," 'number three is displayed here <EOS>',\n"," 'in this image , the number four is shown <EOS>',\n"," 'number five is displayed here <EOS>',\n"," 'in this image , the number one is shown <EOS>',\n"," 'in this image , the number seven is shown <EOS>',\n"," 'number two is displayed here <EOS>',\n"," 'in this image , the number four is shown <EOS>',\n"," 'in this image , the number zero is shown <EOS>']"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["with torch.no_grad():\n","    generated_captions = [model.caption_image(t_img.to(device)) for t_img in random_test_imgs]\n","\n","generated_captions = [' '.join(tokens) for tokens in generated_captions]\n","\n","generated_captions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
