{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEn4bSH5+1PkfHgTo1RnIj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1MPjzbclpNDc","executionInfo":{"status":"ok","timestamp":1711825758445,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","source":["# %pip install torch torchtext tqdm import spacy\n","\n","\n","%pip install --upgrade portalocker\n","\n","clear_output()"],"metadata":{"id":"743Rq6vHpZY4","executionInfo":{"status":"ok","timestamp":1711825770624,"user_tz":-300,"elapsed":11458,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","import torch\n","from torchtext.datasets import IMDB\n","from tqdm import tqdm"],"metadata":{"id":"K78FBTizpuKF","executionInfo":{"status":"ok","timestamp":1711825901781,"user_tz":-300,"elapsed":490,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Content\n","\n","In this notebook, we'll take a look at how we can process text to be fed to models. Specifically, we'll take a look at:\n","\n","1. Removing Stop words from the text\n","2. Tokenizing the text\n","3. Converting the text to numerical form to be used by models\n","\n"],"metadata":{"id":"sBhqPtiApzoF"}},{"cell_type":"markdown","source":["## Downloading the Dataset\n","\n","We will work with the IMDB review dataset\n","\n","It's a sentiment analysis dataset which is built using labelling IMDB reviews of movies as positive or negative"],"metadata":{"id":"YnMJvgQqr6ty"}},{"cell_type":"code","source":["# Download and load the IMDB dataset\n","train_data = IMDB(split=('train'))"],"metadata":{"id":"ek3Inej0pyqi","executionInfo":{"status":"ok","timestamp":1711825775487,"user_tz":-300,"elapsed":1,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Custom text processing functions\n","\n","First, we will take a look at how we can manually write functions for tokenization etc.\n","\n","In the comments, you'll also find some of the limitations of the function (which are intentionally not taken care of for now)"],"metadata":{"id":"SxATq0P2ss2P"}},{"cell_type":"code","source":["# Define a function to remove punctuation.\n","# This is often needed because when tokenizing, punctuation can be a problem\n","# for example, we want only one token for the word \"close\" but if \"close,\" and \"close.\" also exist in text corpus\n","# we will have 3 tokens just representing the word close (\"close\"|\"close,\"|\"close.\")\n","def remove_punctuation(text):\n","    return text.translate(str.maketrans('', '', string.punctuation))\n","\n","# Define a function to remove stop words\n","def remove_stopwords(text):\n","\n","    # The list could be ever expanding. Not to mention, if we dont remove punctuation beforehand, this function will not work properly.\n","    # This is also made to only work with lower case words so we need to lower our text.\n","    stopwords = set([\n","        \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n","        \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n","        \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n","        \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n","        \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n","        \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n","        \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n","        \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n","        \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n","        \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n","    ])\n","    return ' '.join(word for word in text.split() if word.lower() not in stopwords)\n","\n","# Tokenize the text. For now we will just use split over white space.\n","def tokenize(text):\n","    return text.split()\n"],"metadata":{"id":"95ss1oC1pnHj","executionInfo":{"status":"ok","timestamp":1711827439060,"user_tz":-300,"elapsed":820,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["## Processing the IMDB dataset\n","\n","to save time, we will use a small part of the dataset"],"metadata":{"id":"7jqXMgSouRxb"}},{"cell_type":"code","source":["n_samples = 100\n","data = []\n","\n","for record in tqdm(train_data, total=n_samples):\n","\n","    data.append(record[1])  # [0] is the sentiment label. we dont need that.\n","\n","    if len(data) >= n_samples:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFS64ok8ucH5","executionInfo":{"status":"ok","timestamp":1711827444940,"user_tz":-300,"elapsed":503,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"a3d80521-3750-4fbf-d12b-7d2eb4a0fd7f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 99/100 [00:00<00:00, 506.84it/s]\n"]}]},{"cell_type":"code","source":["# Create vocabulary from training data\n","vocab = set()\n","for text in tqdm(data, desc='Creating Vocab'):\n","    text = text.lower()\n","    text = remove_punctuation(text)\n","    text = remove_stopwords(text)\n","    tokens = tokenize(text)\n","    vocab.update(tokens)\n","\n","# Create word to index mapping. This will be used to convert words to numerical form so it can be fed to models\n","# each word will be converted to the integer value stored against it in the dict, which comes from it's index hence word_to_idx\n","word_to_idx = {word: idx for idx, word in tqdm(enumerate(vocab), desc='Building Word 2 Idx')}\n","\n","# Print the first few words and their indices\n","print(\"Word to Index Mapping:\")\n","for word in list(vocab)[:10]:\n","    print(f\"{word}: {word_to_idx[word]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMDWaO4vt1lY","executionInfo":{"status":"ok","timestamp":1711827448806,"user_tz":-300,"elapsed":522,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"ce08be82-c6c2-43df-a42b-ac285db357ed"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Creating Vocab: 100%|██████████| 100/100 [00:00<00:00, 6952.27it/s]\n","Building Word 2 Idx: 4476it [00:00, 844293.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Word to Index Mapping:\n","anything: 0\n","chocolate: 1\n","pointed: 2\n","incompetent: 3\n","catalan: 4\n","ahead: 5\n","incredible: 6\n","dons: 7\n","somebody: 8\n","effectbr: 9\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Lets take a look at an example\n","test_idx = 10\n","\n","text = data[test_idx]\n","\n","processed_text = remove_punctuation(text.lower())\n","processed_text = remove_stopwords(processed_text)\n","tokens = tokenize(processed_text)\n","\n","idxs = [word_to_idx[token] for token in tokens]\n","\n","print('Original Text:')\n","print(text)\n","print('-'*30)\n","print('Tokens:')\n","print(tokens)\n","print('-'*30)\n","print('Numerical form (Idxs)')\n","print(idxs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fnNxrxWy9_3","executionInfo":{"status":"ok","timestamp":1711827454989,"user_tz":-300,"elapsed":515,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"64a2c952-af3f-476e-d4df-7d39d922e2b1"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text:\n","It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn't go on to star in more and better films. Sadly, I didn't think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat's Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich's ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain't no \"Paper Moon\" and only a very pale version of \"What's Up, Doc\".\n","------------------------------\n","Tokens:\n","['great', 'see', 'favorite', 'stars', '30', 'years', 'ago', 'including', 'john', 'ritter', 'ben', 'gazarra', 'audrey', 'hepburn', 'looked', 'quite', 'wonderful', 'given', 'characters', 'good', 'lines', 'work', 'neither', 'understood', 'cared', 'characters', 'doingbr', 'br', 'smaller', 'female', 'roles', 'fine', 'patty', 'henson', 'colleen', 'camp', 'quite', 'competent', 'confident', 'small', 'sidekick', 'parts', 'showed', 'talent', 'sad', 'didnt', 'go', 'star', 'better', 'films', 'sadly', 'didnt', 'think', 'dorothy', 'stratten', 'got', 'chance', 'act', 'important', 'film', 'rolebr', 'br', 'film', 'appears', 'fans', 'openminded', 'started', 'watching', 'big', 'peter', 'bogdanovich', 'fan', 'enjoyed', 'last', 'movie', 'cats', 'meow', 'early', 'ones', 'targets', 'nickleodeon', 'really', 'surprised', 'barely', 'able', 'keep', 'awake', 'watching', 'onebr', 'br', 'ironic', 'movie', 'detective', 'agency', 'detectives', 'clients', 'get', 'romantically', 'involved', 'five', 'years', 'later', 'bogdanovichs', 'exgirlfriend', 'cybil', 'shepherd', 'hit', 'television', 'series', 'called', 'moonlighting', 'stealing', 'story', 'idea', 'bogdanovich', 'course', 'great', 'difference', 'series', 'relied', 'tons', 'witty', 'dialogue', 'tries', 'make', 'slapstick', 'screwball', 'linesbr', 'br', 'bottom', 'line', 'aint', 'paper', 'moon', 'pale', 'version', 'whats', 'doc']\n","------------------------------\n","Numerical form (Idxs)\n","[3635, 1809, 1724, 4326, 1379, 1209, 1481, 272, 2542, 813, 3078, 3664, 1705, 2408, 3659, 2569, 3243, 1201, 229, 1699, 3682, 822, 464, 1752, 4399, 229, 1935, 348, 192, 1482, 153, 721, 1605, 940, 1683, 3574, 2569, 1211, 1553, 3738, 1759, 2302, 2457, 4294, 3766, 993, 2002, 4114, 3865, 3884, 3915, 993, 596, 2730, 985, 702, 4348, 2634, 1989, 4055, 2813, 348, 4055, 1877, 3020, 104, 640, 2519, 4122, 2926, 1785, 3825, 852, 2349, 2491, 782, 4408, 3357, 375, 4212, 2752, 3913, 3298, 1015, 3168, 1833, 2796, 2519, 4189, 348, 4435, 2491, 2510, 2937, 4171, 389, 2621, 247, 1593, 2338, 1209, 3651, 1842, 4180, 3074, 3607, 131, 2590, 249, 3497, 203, 1723, 362, 1150, 1785, 2362, 3635, 3010, 249, 1566, 4246, 2500, 1255, 4299, 2915, 1848, 2028, 132, 348, 2734, 4275, 2213, 801, 3148, 2969, 2392, 1620, 2197]\n"]}]},{"cell_type":"markdown","source":["## Using Spacy\n","\n","As we saw, doing this manually have quite a few limitations.\n","\n","One that hasn't been mentioned above, is that we did a very basic implementation for english, but if we want to work with multiple languages, we need to make these functionalities for them as well, which can be challenging if the developer have limited understanding of the other language\n","\n","We CAN work around them but that adds a lot of needless development overhead. Which is why, it's better to use a library to take care of things like this.\n","\n","The 2 most popular libraries are Spacy and NLTK. In this demo, we will use spacy"],"metadata":{"id":"OlDnxrmp0v3W"}},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"MYrJbdXK1XGk","executionInfo":{"status":"ok","timestamp":1711827006362,"user_tz":-300,"elapsed":1892,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# We need to download the english data and tokenizer for spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"kOwW4yDo1bDf","executionInfo":{"status":"ok","timestamp":1711827010019,"user_tz":-300,"elapsed":1338,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Define a function to remove stop words using spaCy\n","def remove_stopwords_sp(text):  # _sp to keep it seperate from our own implementation.\n","    doc = nlp(text)\n","    return ' '.join(token.text for token in doc if not token.is_stop)\n","\n","# Tokenize the text using spaCy\n","def tokenize_sp(text):\n","    doc = nlp(text)\n","    return [token.text for token in doc]"],"metadata":{"id":"kUbLtNqL0vNA","executionInfo":{"status":"ok","timestamp":1711827696792,"user_tz":-300,"elapsed":511,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Create vocabulary from training data\n","vocab = set()\n","for text in tqdm(data, desc='Creating Vocab'):\n","    text = remove_stopwords_sp(text.lower())  # .lower to maintain similarity with our custom implementation\n","    tokens = tokenize_sp(text)\n","    vocab.update(tokens)\n","\n","# Create word to index mapping\n","word_to_idx = {word: idx for idx, word in tqdm(enumerate(vocab), desc='Building Word 2 Idx')}\n","\n","# Print the first few words and their indices\n","print(\"Word to Index Mapping:\")\n","for word in list(vocab)[:10]:\n","    print(f\"{word}: {word_to_idx[word]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5OkIXVI13Qro","executionInfo":{"status":"ok","timestamp":1711828402350,"user_tz":-300,"elapsed":8997,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"2fbae47f-00e0-49c6-8436-3677770dfcd5"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["Creating Vocab: 100%|██████████| 100/100 [00:08<00:00, 12.03it/s]\n","Building Word 2 Idx: 4331it [00:00, 748630.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Word to Index Mapping:\n","chocolate: 0\n","catalan: 1\n","pointed: 2\n","incompetent: 3\n","mess.i: 4\n","ahead: 5\n","incredible: 6\n","dons: 7\n","somebody: 8\n","ended: 9\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Lets take a look at an example\n","test_idx = 10\n","\n","text = data[test_idx]\n","\n","processed_text = remove_stopwords_sp(text.lower())\n","tokens = tokenize_sp(processed_text)\n","\n","idxs = [word_to_idx[token] for token in tokens]\n","\n","print('Original Text:')\n","print(text)\n","print('-'*30)\n","print('Tokens:')\n","print(tokens)\n","print('-'*30)\n","print('Numerical form (Idxs)')\n","print(idxs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PdT9JzQA5ApS","executionInfo":{"status":"ok","timestamp":1711828410656,"user_tz":-300,"elapsed":511,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"b4879a84-63a0-459f-e00b-da72817c7cf3"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Text:\n","It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn't go on to star in more and better films. Sadly, I didn't think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat's Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich's ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain't no \"Paper Moon\" and only a very pale version of \"What's Up, Doc\".\n","------------------------------\n","Tokens:\n","['great', 'favorite', 'stars', '30', 'years', 'ago', 'including', 'john', 'ritter', ',', 'ben', 'gazarra', 'audrey', 'hepburn', '.', 'looked', 'wonderful', '.', '.', 'given', 'characters', 'good', 'lines', 'work', '.', 'understood', 'cared', 'characters', 'doing.<br', '/><br', '/>some', 'smaller', 'female', 'roles', 'fine', ',', 'patty', 'henson', 'colleen', 'camp', 'competent', 'confident', 'small', 'sidekick', 'parts', '.', 'showed', 'talent', 'sad', 'star', 'better', 'films', '.', 'sadly', ',', 'think', 'dorothy', 'stratten', 'got', 'chance', 'act', 'important', 'film', 'role.<br', '/><br', '/>the', 'film', 'appears', 'fans', ',', 'open', '-', 'minded', 'started', 'watching', '.', 'big', 'peter', 'bogdanovich', 'fan', 'enjoyed', 'movie', ',', '\"', 'cat', 'meow', '\"', 'early', 'ones', '\"', 'targets', '\"', '\"', 'nickleodeon', '\"', '.', ',', 'surprised', 'barely', 'able', 'awake', 'watching', 'one.<br', '/><br', '/>it', 'ironic', 'movie', 'detective', 'agency', 'detectives', 'clients', 'romantically', 'involved', '.', 'years', 'later', ',', 'bogdanovich', 'ex', '-', 'girlfriend', ',', 'cybil', 'shepherd', 'hit', 'television', 'series', 'called', '\"', 'moonlighting', '\"', 'stealing', 'story', 'idea', 'bogdanovich', '.', 'course', ',', 'great', 'difference', 'series', 'relied', 'tons', 'witty', 'dialogue', ',', 'tries', 'slapstick', 'screwball', 'lines.<br', '/><br', '/>bottom', 'line', ':', 'ai', '\"', 'paper', 'moon', '\"', 'pale', 'version', '\"', ',', 'doc', '\"', '.']\n","------------------------------\n","Numerical form (Idxs)\n","[3530, 1665, 4183, 1328, 1156, 1427, 257, 2466, 778, 2601, 2989, 3559, 1647, 2323, 2397, 3555, 3139, 2397, 2397, 1148, 220, 1642, 3577, 787, 2397, 1698, 4254, 220, 3975, 1134, 665, 185, 1428, 153, 689, 2601, 1553, 906, 1624, 3468, 1157, 1504, 3637, 1706, 2222, 2397, 2376, 4151, 3659, 3992, 3756, 3773, 2397, 3800, 2601, 570, 2660, 948, 668, 4206, 2566, 1921, 3938, 456, 1134, 26, 3938, 1817, 2934, 2601, 1280, 3865, 1982, 612, 2444, 2397, 3998, 2844, 1730, 3715, 821, 2417, 2601, 1209, 1345, 4263, 1209, 3254, 357, 1209, 4082, 1209, 1209, 2682, 1209, 2397, 2601, 3192, 974, 3071, 2719, 2444, 1567, 1134, 2803, 4292, 2417, 2436, 2855, 4047, 371, 236, 1543, 2397, 1156, 3547, 2601, 1730, 4287, 3865, 2292, 2601, 2984, 3498, 132, 2515, 238, 3389, 1209, 198, 1209, 1664, 344, 1090, 1730, 2397, 2278, 2601, 3530, 2925, 238, 1515, 4110, 2426, 1197, 2601, 4155, 1789, 1959, 20, 1134, 1825, 4135, 728, 1029, 1209, 766, 3054, 1209, 2884, 2307, 1209, 2601, 2119, 1209, 2397]\n"]}]},{"cell_type":"markdown","source":["### Something to notice:\n","\n","As seen from the examples below, We dont have to remove punctuation. Spacy makes a seperate token for punctuation marks. This lets our data contain punctuation and in theory, should be able to give our model understanding of these punctuation marks."],"metadata":{"id":"rRWsj-J85Y38"}},{"cell_type":"code","source":["tokenize(\"Hi, I Love Machine Learning and I want to be a pro at it.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"De_ValRU4clD","executionInfo":{"status":"ok","timestamp":1711828446818,"user_tz":-300,"elapsed":500,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"5cbb3505-33b5-4a7c-8d2a-de6cfe9042f9"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hi,',\n"," 'I',\n"," 'Love',\n"," 'Machine',\n"," 'Learning',\n"," 'and',\n"," 'I',\n"," 'want',\n"," 'to',\n"," 'be',\n"," 'a',\n"," 'pro',\n"," 'at',\n"," 'it.']"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["tokenize_sp(\"Hi, I Love Machine Learning and I want to be a pro at it.\")  # different tokens for punctuation marks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZlCj2wz4u5U","executionInfo":{"status":"ok","timestamp":1711828448250,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"bccd0559-0045-46e6-87d7-5e9f3c3e66e2"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hi',\n"," ',',\n"," 'I',\n"," 'Love',\n"," 'Machine',\n"," 'Learning',\n"," 'and',\n"," 'I',\n"," 'want',\n"," 'to',\n"," 'be',\n"," 'a',\n"," 'pro',\n"," 'at',\n"," 'it',\n"," '.']"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":[],"metadata":{"id":"KyLWarkR6UmD"},"execution_count":null,"outputs":[]}]}