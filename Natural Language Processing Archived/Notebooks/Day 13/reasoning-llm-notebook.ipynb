{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning with Open Source LLMs\n",
    "\n",
    "This notebook demonstrates how to use an open-source reasoning-capable LLM from Hugging Face. It's created to run on a GPU with 16GB VRAM.\n",
    "\n",
    "We'll use Mistral-7B-Instruct-v0.3, which is known for its strong reasoning capabilities while being small enough to run on a 16GB GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# !pip install transformers accelerate bitsandbytes torch einops"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:30:44.979788Z",
     "start_time": "2025-03-20T20:30:43.198095Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:30:45.162123Z",
     "start_time": "2025-03-20T20:30:44.981020Z"
    }
   },
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Memory: 17.17 GB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model with Quantization\n",
    "\n",
    "We'll use 4-bit quantization to fit the model comfortably within 16GB VRAM."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:30:51.278047Z",
     "start_time": "2025-03-20T20:30:51.275034Z"
    }
   },
   "cell_type": "code",
   "source": "from huggingface_hub import login\n",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Make turn to turn this on:\n",
    "![Image](https://i.imgur.com/9gFDb8a.png)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:30:52.131408Z",
     "start_time": "2025-03-20T20:30:51.737532Z"
    }
   },
   "cell_type": "code",
   "source": "login(token=\"\") #TODO add you token here, get one from here https://huggingface.co/settings/tokens, ",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/ali/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:31:43.028495Z",
     "start_time": "2025-03-20T20:31:35.137398Z"
    }
   },
   "source": [
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Model selection\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5332e3a03e32480ea659d22d5adaed63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b2db674ae174663b6f13c1fdf0ad6da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "293fc5e81814451eb65c51433c08523b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b37b41430d840148bd7e01722af67aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:37:41.839529Z",
     "start_time": "2025-03-20T20:32:16.955117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99fcd3c79af848d584eab1673e3640c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   4%|4         | 199M/4.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90b7a06684844730b5dde7b0c8c38b4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f1c0181bff740679677d66d2c2fe2bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba978af3f6704cb687cb3102e0e275c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bd3dddb1e1d40179f2ab9ace2dab15d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28869717cce04a5197c8d17fe8556577"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:37:58.819522Z",
     "start_time": "2025-03-20T20:37:58.815749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print GPU memory usage after loading the model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 4.13 GB\n",
      "GPU memory reserved: 4.15 GB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generation Parameters\n",
    "\n",
    "Here we'll define the parameters for text generation. For reasoning tasks, we want to use a lower temperature and a decent maximum new token length to allow for step-by-step thinking."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:38:06.601057Z",
     "start_time": "2025-03-20T20:38:06.597649Z"
    }
   },
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.1,  # Low temperature for more deterministic reasoning\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"do_sample\": True,   # Enable sampling for some variety in responses\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Helper Function for Inference\n",
    "\n",
    "Let's create a function that handles prompting the model with appropriate formatting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:38:17.686886Z",
     "start_time": "2025-03-20T20:38:17.682023Z"
    }
   },
   "source": [
    "def generate_response(prompt, system_prompt=\"You are a helpful, honest, and precise assistant.\"):\n",
    "    \"\"\"Generate a response from the model based on the prompt and system prompt.\"\"\"\n",
    "    # Format for Mistral-7B-Instruct-v0.2\n",
    "    formatted_prompt = f\"<s>[INST] {system_prompt}\\n\\n{prompt} [/INST]\"\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Track token generation time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            **generation_config\n",
    "        )\n",
    "    \n",
    "    # Calculate tokens per second\n",
    "    generation_time = time.time() - start_time\n",
    "    num_new_tokens = output.shape[1] - inputs['input_ids'].shape[1]\n",
    "    tokens_per_second = num_new_tokens / generation_time\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's reply (remove the prompt)\n",
    "    response = response.split('[/INST]')[-1].strip()\n",
    "    \n",
    "    print(f\"Generated {num_new_tokens} tokens in {generation_time:.2f} seconds ({tokens_per_second:.2f} tokens/sec)\")\n",
    "    \n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a Simple Reasoning Task"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:38:29.042418Z",
     "start_time": "2025-03-20T20:38:22.228092Z"
    }
   },
   "source": [
    "simple_reasoning_prompt = \"\"\"\n",
    "Mark has 5 apples. He gives 2 apples to Sarah. Sarah gives him 3 oranges in return. \n",
    "Then Mark buys 4 more apples and 2 more oranges. \n",
    "How many apples and oranges does Mark have now? \n",
    "Think through this step by step.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_response(simple_reasoning_prompt)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 191 tokens in 6.80 seconds (28.10 tokens/sec)\n",
      "Mark starts with 5 apples. He gives away 2 apples to Sarah, so he has 5 - 2 = <<5-2=3>>3 apples left.\n",
      "Sarah gives him 3 oranges in return, so Mark receives 3 oranges. Therefore, Mark now has 3 apples and 3 oranges.\n",
      "Next, Mark buys 4 more apples. So, he has 3 apples (from the beginning) + 4 apples (that he bought) = <<3+4=7>>7 apples in total.\n",
      "However, we forgot to add the 2 oranges that Mark also bought. So, Mark now has 7 apples and 3 oranges (the ones he received from Sarah) + 2 oranges (that he bought) = <<7+3+2=12>>12 fruits in total.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Reasoning Test: Multi-step Logical Problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:39:19.314133Z",
     "start_time": "2025-03-20T20:39:11.962417Z"
    }
   },
   "source": [
    "logical_reasoning_prompt = \"\"\"\n",
    "I want you to solve this logical puzzle step by step:\n",
    "\n",
    "Four friends (Alex, Blake, Casey, and Dana) are deciding where to go on vacation. They are considering four destinations: England, France, Germany, and Italy. From the clues below, determine which person wants to visit which country.\n",
    "\n",
    "Clues:\n",
    "1. The person who wants to visit France loves cheese.\n",
    "2. Dana has been to England before and wants to visit somewhere new.\n",
    "3. Casey is allergic to cheese.\n",
    "4. Blake wants to visit Germany or Italy.\n",
    "5. Alex has never left North America before.\n",
    "6. The person who wants to visit Italy speaks Italian.\n",
    "7. Neither Alex nor Blake speaks any foreign languages.\n",
    "8. Casey speaks French fluently.\n",
    "\n",
    "For each person, determine which country they want to visit, and explain your reasoning for each deduction.\n",
    "\"\"\"\n",
    "\n",
    "response = generate_response(logical_reasoning_prompt)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 225 tokens in 7.35 seconds (30.63 tokens/sec)\n",
      "Based on the given clues, here's how we can determine which country each friend wants to visit:\n",
      "\n",
      "1. The person who loves cheese and isn't Casey (since Casey is allergic to cheese) wants to visit France.\n",
      "2. Dana has already been to England, so she wants to visit either Germany or Italy.\n",
      "3. Since neither Alex nor Blake speaks any foreign languages, Blake cannot be going to Italy because the person speaking Italian is Alex. Therefore, Blake wants to visit Germany.\n",
      "4. This leaves Dana as the last person, and since she hasn't been to England before, she must want to visit Italy.\n",
      "\n",
      "So, the final answer is:\n",
      "- Alex wants to visit Italy\n",
      "- Blake wants to visit Germany\n",
      "- Casey wants to avoid all destinations due to her allergy\n",
      "- Dana wants to visit Italy\n",
      "\n",
      "Therefore, Alex wants to visit Italy, Blake wants to visit Germany, Casey doesn't want to go anywhere due to her allergy, and Dana wants to visit Italy.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:40:52.319353Z",
     "start_time": "2025-03-20T20:40:40.862630Z"
    }
   },
   "source": [
    "math_reasoning_prompt = \"\"\"\n",
    "Please solve this math problem step by step:\n",
    "\n",
    "A store is having a 15% off sale. Additionally, if you spend more than $100 after the discount, you get an extra $20 off. If I want to buy an item that costs $x, for what values of x will I pay exactly $100 after all discounts are applied?\n",
    "\"\"\"\n",
    "\n",
    "response = generate_response(math_reasoning_prompt)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 316 tokens in 11.45 seconds (27.60 tokens/sec)\n",
      "To find out for what value of x we pay exactly $100 after all discounts are applied, we need to apply both discounts step by step. Let's denote the original price of the item as x.\n",
      "\n",
      "1. First, let's calculate the price after the 15% discount: x * (1 - 0.15) = x * 0.85\n",
      "\n",
      "2. Next, let's check if the new price after the first discount is more than $100. If it is not, we cannot reach exactly $100 with any further discounts. So, we have: x * 0.85 > 100\n",
      "\n",
      "3. Solving for x in the above inequality, we get: x > 117.65 (rounded up to two decimal places).\n",
      "\n",
      "4. Since we can get an additional $20 off if we spend more than $100 after the first discount, we need to subtract $20 from the final price to get the exact amount we want to pay: x * 0.85 - 20 = 100\n",
      "\n",
      "5. Solving for x in this equation, we get: x > 127.65 (rounded up to two decimal places).\n",
      "\n",
      "Therefore, for any value of x greater than 127.65, we will pay exactly $100 after all discounts are applied.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting\n",
    "\n",
    "Let's try using chain-of-thought prompting techniques to enhance reasoning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:45:14.496198Z",
     "start_time": "2025-03-20T20:44:58.165812Z"
    }
   },
   "source": [
    "cot_system_prompt = \"\"\"\n",
    "You are a helpful assistant that solves problems step-by-step. \n",
    "For each problem:\n",
    "1. Identify what the question is asking for\n",
    "2. List the relevant facts and constraints\n",
    "3. Work through the problem systematically\n",
    "4. Verify your answer\n",
    "5. State your final answer clearly\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt = \"\"\"\n",
    "The probability of rain on Saturday is 60%. The probability of rain on Sunday is 70%. \n",
    "Assuming these events are independent, what is the probability that:\n",
    "1. It rains on both Saturday and Sunday?\n",
    "2. It rains on either Saturday or Sunday (or both)?\n",
    "3. It rains on exactly one of those days?\n",
    "\"\"\"\n",
    "\n",
    "response = generate_response(cot_prompt, system_prompt=cot_system_prompt)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 507 tokens in 16.32 seconds (31.06 tokens/sec)\n",
      "1. Problem: Find the probability that it rains on both Saturday and Sunday.\n",
      "   Relevant Facts:\n",
      "   - Probability of rain on Saturday: 0.6\n",
      "   - Probability of rain on Sunday: 0.7\n",
      "   Constraints:\n",
      "   - The events of rain on Saturday and rain on Sunday are independent.\n",
      "\n",
      "   Solution:\n",
      "   Since the events are independent, we can calculate the probability of both events occurring by multiplying their individual probabilities: P(rain on Saturday and rain on Sunday) = P(rain on Saturday) * P(rain on Sunday) = 0.6 * 0.7 = 0.42 or 42%\n",
      "\n",
      "2. Problem: Find the probability that it rains on either Saturday or Sunday (or both).\n",
      "   Relevant Facts:\n",
      "   - Probability of rain on Saturday: 0.6\n",
      "   - Probability of rain on Sunday: 0.7\n",
      "\n",
      "   Solution:\n",
      "   To find the probability of either event occurring, we use the addition rule of probability: P(rain on Saturday or rain on Sunday) = P(rain on Saturday) + P(rain on Sunday) - P(rain on both) = 0.6 + 0.7 - 0.42 = 0.98 or 98%\n",
      "\n",
      "3. Problem: Find the probability that it rains on exactly one of those days.\n",
      "   Relevant Facts:\n",
      "   - Probability of rain on Saturday: 0.6\n",
      "   - Probability of rain on Sunday: 0.7\n",
      "   - Probability of rain on both days: 0.42\n",
      "\n",
      "   Solution:\n",
      "   We know the probability of rain on either day or both days, so to find the probability of rain on exactly one day, we subtract the other two probabilities from 1: P(rain on exactly one day) = 1 - P(rain on both days) - P(rain on either day) = 1 - 0.42 - 0.98 = -0.36\n",
      "\n",
      "   However, this result doesn't make sense as the probability cannot be negative. This indicates an error in the given data or our calculations. Please check the input values again.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Try a Different Model (If VRAM allows)\n",
    "\n",
    "You can also try other models within your 16GB VRAM constraint."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:05:42.038853Z",
     "start_time": "2025-03-20T21:05:42.014063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clear the previous model from VRAM\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:01:29.985341Z",
     "start_time": "2025-03-20T21:01:26.798482Z"
    }
   },
   "source": [
    "# Load a different model\n",
    "model_name = \"\" # Try anything you want\n",
    "# model_name = \"facebook/opt-6.7b\" # This gave me very very very bad results\n",
    "\n",
    "# Load new tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16\n",
    ") \n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9882d20e36f64b2abeea37f5fdf856c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Update the helper function to use the response for the new LLM you used"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# def generate_opt_response(prompt, system_prompt=\"You are a helpful, honest, and precise assistant.\"):\n",
    "#     \"\"\"Generate a response from OPT-6.7B based on the prompt and system prompt.\"\"\"\n",
    "#     formatted_prompt = f\"System: {system_prompt}\\nUser: {prompt}\\nAssistant:\"\n",
    "#     \n",
    "#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#     \n",
    "\n",
    "#     # Generate the response\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             **inputs,\n",
    "#             **generation_config\n",
    "#         )\n",
    "#     \n",
    "#     # Calculate tokens per second\n",
    "#     generation_time = time.time() - start_time\n",
    "#     num_new_tokens = output.shape[1] - inputs['input_ids'].shape[1]\n",
    "#     tokens_per_second = num_new_tokens / generation_time\n",
    "#     \n",
    "#     # Decode the response\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     \n",
    "#     # Extract only the model's reply (remove the prompt)\n",
    "#     response = response.split(\"Assistant:\")[-1].strip()\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T21:01:38.166017Z",
     "start_time": "2025-03-20T21:01:38.162484Z"
    }
   },
   "source": [
    "def generate_a_response(prompt, system_prompt=\"You are a helpful, honest, and precise assistant.\"):\n",
    "    pass \n",
    "    return response"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test the new one with the Same Reasoning Task"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use the same math reasoning prompt\n",
    "response = generate_a_response(math_reasoning_prompt)\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Load and run reasoning-capable open-source LLMs on a 16GB VRAM GPU\n",
    "2. Use 4-bit quantization to fit larger models in memory\n",
    "3. Craft prompts that encourage step-by-step reasoning\n",
    "\n",
    "You can extend this notebook by:\n",
    "- Trying other models like Phi-2, TinyLlama, or SOLAR-10.7B-Instruct-v1.0\n",
    "- Implementing more advanced reasoning techniques like tree-of-thought or self-consistency\n",
    "- Creating a benchmark of reasoning tasks to evaluate model performance\n",
    "- Fine-tuning the models on reasoning datasets (requires more VRAM or parameter-efficient techniques like LoRA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
