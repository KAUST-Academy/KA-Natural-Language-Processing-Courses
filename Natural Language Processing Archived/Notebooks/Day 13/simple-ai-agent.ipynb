{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AI Agent with Web Search and HuggingFace Integration\n",
    "\n",
    "This notebook implements a simple AI agent that can:\n",
    "1. Search the internet for information using free tools\n",
    "2. Use open-source Hugging Face models for text generation\n",
    "3. Combine these capabilities to answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "Let's install the required libraries first."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "# !pip install transformers torch requests beautifulsoup4 duckduckgo-search wikipedia"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "import wikipedia\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import time\n",
    "from transformers import BitsAndBytesConfig\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Tools\n",
    "\n",
    "First, let's implement tools to search the web without requiring API keys."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class WebSearchTools:\n",
    "    \"\"\"Collection of web search tools that don't require API keys\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def duckduckgo_search(query: str, num_results: int = 5) -> List[Dict[str, str]]:\n",
    "        \"\"\"Search the web using DuckDuckGo\"\"\"\n",
    "        try:\n",
    "            with DDGS() as ddgs:\n",
    "                results = []\n",
    "                for r in ddgs.text(query, max_results=num_results):\n",
    "                    results.append({\n",
    "                        'title': r['title'],\n",
    "                        'link': r['href'],\n",
    "                        'snippet': r['body']\n",
    "                    })\n",
    "                return results\n",
    "        except Exception as e:\n",
    "            print(f\"DuckDuckGo search error: {e}\")\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_webpage_content(url: str) -> str:\n",
    "        \"\"\"Fetch and extract text from a webpage\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "\n",
    "            # Get text\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "            # Clean text (remove extra whitespace)\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "            return text[:5000]  # Limit to first 5000 chars to avoid huge texts\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching webpage: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def wikipedia_search(query: str, sentences: int = 3) -> str:\n",
    "        \"\"\"Search Wikipedia for information\"\"\"\n",
    "        try:\n",
    "            # Search for the page\n",
    "            search_results = wikipedia.search(query)\n",
    "            if not search_results:\n",
    "                return \"\"\n",
    "\n",
    "            # Get the page\n",
    "            try:\n",
    "                page = wikipedia.page(search_results[0])\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                # If there's a disambiguation page, take the first option\n",
    "                page = wikipedia.page(e.options[0])\n",
    "\n",
    "            # Get a summary\n",
    "            summary = wikipedia.summary(page.title, sentences=sentences)\n",
    "            return f\"Wikipedia ({page.title}): {summary}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia search error: {e}\")\n",
    "            return \"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Model Interface\n",
    "\n",
    "Now, let's create a class to work with open-source Hugging Face models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class HuggingFaceModel:\n",
    "    \"\"\"Interface for working with HuggingFace models\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "        \"\"\"Initialize the model and tokenizer\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model ID\n",
    "        \"\"\"\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "\n",
    "        # Check if CUDA is available\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # For faster loading with limited resources (8-bit quantization)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "\n",
    "        # Configure quantization if using CUDA\n",
    "        if self.device == \"cuda\":\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=True\n",
    "            )\n",
    "        else:\n",
    "            quantization_config = None\n",
    "            \n",
    "        # Load the model with the new configuration\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "\n",
    "        # Create a text generation pipeline\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def generate_text(self, prompt: str, max_new_tokens: int = 512) -> str:\n",
    "        \"\"\"Generate text based on a prompt\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input text to the model\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            outputs = self.generator(\n",
    "                prompt,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "            # Extract generated text, excluding the prompt\n",
    "            generated_text = outputs[0]['generated_text']\n",
    "\n",
    "            # Remove the original prompt from the output\n",
    "            if generated_text.startswith(prompt):\n",
    "                generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Text generation error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_response_with_context(self, question: str, context: str, max_new_tokens: int = 1024) -> str:\n",
    "        \"\"\"Generate a response to a question given additional context\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            context: Additional context information\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"<s>[INST] I have the following information: \n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Based on this information, please answer the following question:\n",
    "    {question} [/INST]\n",
    "    \"\"\"\n",
    "        return self.generate_text(prompt, max_new_tokens=max_new_tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple AI Agent\n",
    "\n",
    "Now, let's integrate everything to create our simple AI agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleAIAgent:\n",
    "    \"\"\"A simple AI agent that can search the web and use Hugging Face models\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "        \"\"\"Initialize the agent with search tools and a language model\"\"\"\n",
    "        self.search_tools = WebSearchTools()\n",
    "        self.llm = HuggingFaceModel(model_name)\n",
    "\n",
    "    def search(self, query: str) -> tuple:\n",
    "        \"\"\"Search for information using multiple sources\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (formatted_context, raw_results)\n",
    "        \"\"\"\n",
    "        print(f\"Searching for information about: {query}\")\n",
    "\n",
    "        # Try Wikipedia first\n",
    "        wiki_result = self.search_tools.wikipedia_search(query, sentences=5)\n",
    "\n",
    "        # Try DuckDuckGo search\n",
    "        ddg_results = self.search_tools.duckduckgo_search(query, num_results=3)\n",
    "\n",
    "        # Combine information\n",
    "        collected_info = []\n",
    "        raw_results = {\n",
    "            \"wikipedia\": wiki_result,\n",
    "            \"duckduckgo\": ddg_results,\n",
    "            \"webpage_content\": None\n",
    "        }\n",
    "\n",
    "        if wiki_result:\n",
    "            collected_info.append(wiki_result)\n",
    "\n",
    "        for result in ddg_results:\n",
    "            collected_info.append(f\"Title: {result['title']}\")\n",
    "            collected_info.append(f\"Snippet: {result['snippet']}\")\n",
    "\n",
    "            # Optionally fetch full content from the first result\n",
    "            if result == ddg_results[0]:\n",
    "                try:\n",
    "                    content = self.search_tools.fetch_webpage_content(result['link'])\n",
    "                    if content:\n",
    "                        # Truncate to avoid very long context\n",
    "                        collected_info.append(f\"Content: {content[:1500]}...\")\n",
    "                        raw_results[\"webpage_content\"] = {\n",
    "                            \"url\": result['link'],\n",
    "                            \"content\": content\n",
    "                        }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching content: {e}\")\n",
    "\n",
    "        return \"\\n\\n\".join(collected_info), raw_results\n",
    "\n",
    "    def answer_question(self, question: str, max_new_tokens: int = 1024, return_search_results: bool = False) -> dict:\n",
    "        \"\"\"Answer a question by searching for information and generating a response\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            return_search_results: Whether to return search results along with the answer\n",
    "            \n",
    "        Returns:\n",
    "            If return_search_results is True, returns a dictionary with answer and search results\n",
    "            Otherwise, returns just the answer string\n",
    "        \"\"\"\n",
    "        # First, generate a good search query from the question\n",
    "        search_query_prompt = f\"\"\"<s>[INST] Convert the following question into a search query with only keywords. \n",
    "    Do not use special search operators, just extract 3-5 key terms:\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Search query: [/INST]\n",
    "    \"\"\"\n",
    "        search_query = self.llm.generate_text(search_query_prompt).strip()\n",
    "\n",
    "        # Remove any extra formatting the model might have added\n",
    "        search_query = re.sub(r'[\\\"\\\\[\\\\]]', '', search_query).strip()\n",
    "        print(f\"Generated search query: {search_query}\")\n",
    "\n",
    "        # Search for information\n",
    "        formatted_context, raw_results = self.search(search_query)\n",
    "\n",
    "        # Generate answer based on the collected information\n",
    "        answer = self.llm.generate_response_with_context(question, formatted_context, max_new_tokens=max_new_tokens)\n",
    "\n",
    "        if return_search_results:\n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"search_query\": search_query,\n",
    "                \"formatted_context\": formatted_context,\n",
    "                \"raw_results\": raw_results\n",
    "            }\n",
    "        else:\n",
    "            return answer\n",
    "\n",
    "    def compare_answers(self, question: str, max_new_tokens: int = 1024) -> dict:\n",
    "        \"\"\"\n",
    "        Compare answers generated with and without web search results\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing both answers and search information\n",
    "        \"\"\"\n",
    "        print(f\"Comparing answers for question: {question}\")\n",
    "\n",
    "        # First, get answer with search results\n",
    "        print(\"Generating answer WITH search...\")\n",
    "        start_time_with_search = time.time()\n",
    "        result_with_search = self.answer_question(question,\n",
    "                                                  max_new_tokens=max_new_tokens,\n",
    "                                                  return_search_results=True)\n",
    "        time_with_search = time.time() - start_time_with_search\n",
    "\n",
    "        # Now, generate an answer without search (direct model response)\n",
    "        print(\"Generating answer WITHOUT search...\")\n",
    "        start_time_without_search = time.time()\n",
    "\n",
    "        # Create a prompt for direct question answering\n",
    "        direct_prompt = f\"\"\"<s>[INST] Please answer the following question based on your knowledge:\n",
    "        \n",
    "        {question} [/INST]\n",
    "        \"\"\"\n",
    "\n",
    "        # Get model's direct response without web search\n",
    "        answer_without_search = self.llm.generate_text(direct_prompt, max_new_tokens=max_new_tokens)\n",
    "        time_without_search = time.time() - start_time_without_search\n",
    "\n",
    "        # Prepare the comparison results\n",
    "        comparison = {\n",
    "            \"question\": question,\n",
    "            \"with_search\": {\n",
    "                \"answer\": result_with_search[\"answer\"],\n",
    "                \"search_query\": result_with_search[\"search_query\"],\n",
    "                \"search_results_summary\": result_with_search[\"formatted_context\"],\n",
    "                \"time_taken\": time_with_search\n",
    "            },\n",
    "            \"without_search\": {\n",
    "                \"answer\": answer_without_search,\n",
    "                \"time_taken\": time_without_search\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    def display_comparison(self, question: str, max_new_tokens: int = 1024):\n",
    "        \"\"\"\n",
    "        Display a side-by-side comparison of answers with and without search\n",
    "        \n",
    "        Args:\n",
    "            question: User's question\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "        \"\"\"\n",
    "        comparison = self.compare_answers(question, max_new_tokens)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"QUESTION: {comparison['question']}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 35 + \" WITH SEARCH \" + \"-\" * 35)\n",
    "        print(f\"Search Query: {comparison['with_search']['search_query']}\")\n",
    "        print(f\"Time: {comparison['with_search']['time_taken']:.2f} seconds\")\n",
    "        print(\"\\nANSWER WITH SEARCH:\")\n",
    "        print(comparison['with_search']['answer'])\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 35 + \" WITHOUT SEARCH \" + \"-\" * 35)\n",
    "        print(f\"Time: {comparison['without_search']['time_taken']:.2f} seconds\")\n",
    "        print(\"\\nANSWER WITHOUT SEARCH:\")\n",
    "        print(comparison['without_search']['answer'])\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 35 + \" SEARCH RESULTS \" + \"-\" * 35)\n",
    "        print(comparison['with_search']['search_results_summary'][:1500] +\n",
    "              \"...\" if len(comparison['with_search']['search_results_summary']) > 1500\n",
    "              else comparison['with_search']['search_results_summary'])\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "        return comparison"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Test the Agent\n",
    "\n",
    "Let's create our agent and test it with some questions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "agent = SimpleAIAgent(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"Who is the current president of the united states?\"\n",
    "detailed_answer = agent.answer_question(question, max_new_tokens=2048)\n",
    "print(f\"Detailed Answer: {detailed_answer}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "question = \"Who is the current president of the united states?\"\n",
    "result = agent.answer_question(question, return_search_results=True, max_new_tokens=2048)\n",
    "\n",
    "print(\"\\n----- SEARCH QUERY -----\")\n",
    "print(result['search_query'])\n",
    "\n",
    "print(\"\\n----- SEARCH RESULTS -----\")\n",
    "print(result['formatted_context'])\n",
    "\n",
    "print(\"\\n----- ANSWER -----\")\n",
    "print(result['answer'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = \"Who is the current president of the united states?\"\n",
    "comparison = agent.display_comparison(question)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
