{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPomR91qptJvbKoObaaTTWQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from IPython.display import clear_output"],"metadata":{"id":"KWJQkUiwOhDk","executionInfo":{"status":"ok","timestamp":1711847386867,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# %pip install nltk tqdm\n","\n","clear_output()"],"metadata":{"id":"go_txTCnOllo","executionInfo":{"status":"ok","timestamp":1711847389194,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Content:\n","\n","In this demo, we will build an N-gram probabilistic based Language model\n","\n","We will use NLTK library to download the dataset and handle our text."],"metadata":{"id":"xNn4k_1tO8wJ"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XdTwtMLKd1H","executionInfo":{"status":"ok","timestamp":1711847453382,"user_tz":-300,"elapsed":1386,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"9b6eb18c-1708-4d1c-cf2b-25b357e0e5fe"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import random\n","\n","import nltk\n","from nltk.util import ngrams as build_ngrams\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","from tqdm import tqdm\n","\n","nltk.download('punkt')"]},{"cell_type":"markdown","source":["## Downloading the dataset"],"metadata":{"id":"A44rzQ3RPkqX"}},{"cell_type":"code","source":["# Download the IMDB dataset\n","nltk.download('movie_reviews')\n","from nltk.corpus import movie_reviews"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_l14RpFVOSPm","executionInfo":{"status":"ok","timestamp":1711847454689,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"924ab151-d0a1-4355-fb48-c52b58fa3639"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n"]}]},{"cell_type":"code","source":["tokenized_data = movie_reviews.sents()  # sents is sentences (not full reviews as can be very long). They are alread tokenized."],"metadata":{"id":"Gxv7nlqJ5Hzf","executionInfo":{"status":"ok","timestamp":1711847528042,"user_tz":-300,"elapsed":1,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Building the model"],"metadata":{"id":"DoyvrQ3cXBDW"}},{"cell_type":"code","source":["sos_token = '<SOS>'  # start of sentence token. Appending this at the start will make the selection of first token also probabilistic based according to corpus\n","eos_token = '<EOS>'  # to indicate a sentence has ended and we should stop generating"],"metadata":{"id":"jhVXSKpWVRc9","executionInfo":{"status":"ok","timestamp":1711847529566,"user_tz":-300,"elapsed":2,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class NGramLanguageModel():\n","\n","    def __init__(self, n):\n","\n","        self.n = n\n","        self.word_freqs = defaultdict(dict) # 1 to n-1 grams: {dict of possible words: frequency}\n","\n","    def train(self, sentences):\n","\n","        for sentence in tqdm(sentences, desc='Processing Sentences'):\n","\n","            for gram_size in range(2, self.n+1):  # 2 because we need to make key(gram[:-1]) val(gram[-1]) pairs and need atleast 2.\n","\n","                ngrams = build_ngrams([sos_token]+sentence+[eos_token], gram_size)  # need to manually append eos_token at the end of sentences\n","\n","                for ngram in ngrams:\n","\n","                    key = ngram[:-1]\n","                    value = ngram[-1]\n","\n","                    self.word_freqs[key][value] = self.word_freqs[key].get(value, 0)+1  # if key doesn't exist already then freq is 0. Whatever the frequency is, add 1 to it.\n","\n","    def generate_sentence(self, starting_state=None, max_length=50):\n","\n","        generated_sentence = []\n","\n","        if starting_state is None:\n","            generated_sentence = [sos_token]\n","        elif isinstance(starting_state, str):\n","            generated_sentence = starting_state.split()\n","\n","        if generated_sentence[0] != sos_token:\n","            generated_sentence = [sos_token]+generated_sentence\n","\n","        max_key_len = self.n-1\n","\n","        if tuple(generated_sentence[-1:]) not in self.word_freqs:  # python automatically takes care of the case if the max_key_len is bigger than total list size\n","            raise ValueError('Invalid starting state')\n","\n","        while len(generated_sentence) <= max_length:\n","\n","            for key_len in range(max_key_len, 0, -1):  # for loop for the condition: if we can't find a combination of lets say the latest 5 gram in corpus, we go for 4 then 3 and so on\n","                last_tokens = generated_sentence[-key_len:]\n","                next_word_freqs = self.word_freqs[tuple(last_tokens)]\n","                if len(next_word_freqs) > 0:\n","                    break\n","\n","            words, freqs = list(zip(*next_word_freqs.items()))  # [words...], [freqs..]\n","\n","            next_word = random.choices(words, weights=freqs, k=1)[0]  # no need to divide and convert to probability first. choices() can take weights as it is.\n","            generated_sentence.append(next_word)\n","\n","            if next_word == eos_token:\n","                break\n","\n","        return generated_sentence\n"],"metadata":{"id":"nMW8F-DPP2ph","executionInfo":{"status":"ok","timestamp":1711847530146,"user_tz":-300,"elapsed":1,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = NGramLanguageModel(n=5)  # The bigger the value of n, the better but bigger the model."],"metadata":{"id":"nqVM2lzvUUmX","executionInfo":{"status":"ok","timestamp":1711847670458,"user_tz":-300,"elapsed":1,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.train(tokenized_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fojAi4QIXUdP","executionInfo":{"status":"ok","timestamp":1711847683237,"user_tz":-300,"elapsed":10662,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"a4870a9c-9895-4ac0-a43f-29a9a1a15a57"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing Sentences: 100%|██████████| 71532/71532 [00:10<00:00, 6587.67it/s]\n"]}]},{"cell_type":"markdown","source":["## Let's see the results"],"metadata":{"id":"7Ts62f2El5qa"}},{"cell_type":"code","source":["# 5 completely random sentences\n","for _ in range(5):\n","    sentence_tokens = model.generate_sentence()\n","    print(' '.join(sentence_tokens))  # skip SOS and EOS tokens\n","    print('-'*20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S68EQoKbXem3","executionInfo":{"status":"ok","timestamp":1711847685162,"user_tz":-300,"elapsed":339,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"c63fabd7-9b92-4d44-81b5-814d1a8219b0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["<SOS> instead , he turns out to be a bad thing ? <EOS>\n","--------------------\n","<SOS> jeff has a good job , and the cast was left to carry the movie , which is frequently on display in lines like ` i ' m gonna start drinking again after this lame - ass movie . <EOS>\n","--------------------\n","<SOS> having not read the novel by thomas hardy , \" jude the obscure , \" his final novel ( final because this film created such an outrage that he never wrote again - see : i did do some research on it ) about a man who was being typecast\n","--------------------\n","<SOS> there are several subplots involving alan . <EOS>\n","--------------------\n","<SOS> nothing is what it seems in this reality , it ' s a rewarding experience . <EOS>\n","--------------------\n"]}]},{"cell_type":"code","source":["model.generate_sentence(starting_state='I felt like')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_oGV0R6faq3","executionInfo":{"status":"ok","timestamp":1711847766198,"user_tz":-300,"elapsed":341,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"}},"outputId":"ca824991-4362-4da2-c638-6b6537b4de65"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<SOS>',\n"," 'I',\n"," 'felt',\n"," 'like',\n"," 'a',\n"," 'slap',\n"," 'in',\n"," 'the',\n"," 'face',\n"," 'to',\n"," 'be',\n"," 'hand',\n"," '-',\n"," 'fed',\n"," 'a',\n"," 'theme',\n"," 'in',\n"," 'such',\n"," 'a',\n"," 'simplistic',\n"," 'way',\n"," '.',\n"," '<EOS>']"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"ixPQ286T_7Ab"},"execution_count":null,"outputs":[]}]}