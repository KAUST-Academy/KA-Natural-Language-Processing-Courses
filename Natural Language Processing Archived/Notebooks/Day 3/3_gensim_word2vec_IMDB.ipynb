{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":571,"status":"ok","timestamp":1712522054362,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"1Kyq7id3RFlx"},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9402,"status":"ok","timestamp":1712522562295,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"sIFHl7ibRLEt"},"outputs":[],"source":["# %pip install gensim nltk tqdm scikit-learn\n","\n","%pip install datasets\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"L6qBaPbbRZI-"},"source":["# Content\n","\n","In this demo, we will train a word2vec model on custom data. We will use IMDB dataset as our training data\n","\n","a word2vec model is another way to convert strings(or words) to numerical represntation, so those vectors can then be used to perform some task or train another model.\n","\n","A word2vec uses Continuous bag of words technique to learn embeddings of words it sees during training, and then saves them. After training, we can infer those vectors by providing the word. We will not delve into details about contiuous bag of words (CBOW) here.\n","\n","The special thing about word2vec vectors (which is not found in TF-IDF or word to index) is that the distance in word2vec vectors (between each other) also represent information about how similar the words are to each other\n","\n","for example, the vectors of words \"cat\" and \"dog\" will have a higher similarity (or lower distance) compared to the vectors of words \"cat\" and \"building\"\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1712523481624,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"aTPmrcboRP1c"},"outputs":[],"source":["import nltk\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from datasets import load_dataset\n","\n","\n","nltk.download('punkt')\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"G1aPCG39TOd6"},"source":["## Preparing the data"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21352,"status":"ok","timestamp":1712522629642,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"DejqTT0YRDZy","outputId":"cf9f4e67-c268-4487-f301-a582223b3f64"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 25000/25000 [00:21<00:00, 1176.71it/s]\n"]}],"source":["train_data = load_dataset('imdb', split='train')['text']\n","train_data = train_data[:500]  # shorten the data because all 25k rows take too long to train"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10366,"status":"ok","timestamp":1712522724824,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"vgYVGGxnTTsn","outputId":"a427a80a-c0a5-4646-8353-8d51b4d65743"},"outputs":[{"name":"stderr","output_type":"stream","text":["Tokenizing data: 100%|██████████| 5000/5000 [00:10<00:00, 470.89it/s]\n"]}],"source":["tokenized_train_data = [word_tokenize(text.lower()) for text in tqdm(train_data, desc='Tokenizing data')]"]},{"cell_type":"markdown","metadata":{"id":"YKWZHhUiUBiw"},"source":["## Train the model"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12681,"status":"ok","timestamp":1712523178264,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"cSyPNnntTtiX"},"outputs":[],"source":["# with tqdm(total=len(tokenized_train_data), desc='Training model') as pbar:\n","model = Word2Vec(tokenized_train_data, vector_size=128, window=5, min_count=1, workers=4)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":397,"status":"ok","timestamp":1712523206260,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"eP6xUg99VFv9"},"outputs":[],"source":["word_vectors = model.wv"]},{"cell_type":"markdown","metadata":{"id":"XiFtgK5xVoSY"},"source":["## Try the model"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1712524649184,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Q5RG-v7cVliT"},"outputs":[],"source":["test_words = ['cat', 'dog', 'building', 'movie', 'action', 'comedy', 'fight', 'plot', 'laugh']\n","test_word_vectors = [word_vectors[word] for word in test_words]\n","\n","test_words_similarity_map = {}  # map each word to all other words where the other words list is sorted according to similarity (most similar first)\n","\n","for i, word in enumerate(test_words):\n","\n","    remaining_words = test_words[:i]+test_words[i+1:]\n","    remaining_word_vectors = test_word_vectors[:i]+test_word_vectors[i+1:]\n","\n","    word_to_remaining_cosine_sims = cosine_similarity(test_word_vectors[i].reshape(1, -1), remaining_word_vectors)[0]\n","    sorting_order = word_to_remaining_cosine_sims.argsort()[::-1]  # [::-1] because argsort returns lowest to highest and we want highest to lowest because higher cosine sim is more similar word\n","    ordered_remaining_words = [remaining_words[j] for j in sorting_order]  # ordered according to similarity with the word(in loop iteration)\n","\n","    test_words_similarity_map[word] = list(zip(ordered_remaining_words, word_to_remaining_cosine_sims[sorting_order]))"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1712524615624,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Uzl7LMrXVumz","outputId":"81814a1b-cf6e-49c1-8f66-00eac59833bd"},"outputs":[{"data":{"text/plain":["[('dog', 0.907266),\n"," ('building', 0.8194644),\n"," ('guns', 0.79736876),\n"," ('fight', 0.79011095),\n"," ('action', 0.45516294),\n"," ('comedy', 0.34774196),\n"," ('plot', 0.15761659),\n"," ('movie', 0.07604584)]"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["test_words_similarity_map['cat']"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1712524616152,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"CdpJ6uPHVwCM","outputId":"93676454-29fa-4956-88a6-ab0c5446a283"},"outputs":[{"data":{"text/plain":["[('plot', 0.7116974),\n"," ('comedy', 0.7073522),\n"," ('fight', 0.49309626),\n"," ('cat', 0.45516294),\n"," ('dog', 0.43521827),\n"," ('movie', 0.36971736),\n"," ('guns', 0.3668394),\n"," ('building', 0.3169372)]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["test_words_similarity_map['action']"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1712524675056,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"GMa4OnldXqdH","outputId":"753c4572-490a-41c9-b3c0-5fdc37f67972"},"outputs":[{"data":{"text/plain":["[('action', 0.7073522),\n"," ('plot', 0.6685944),\n"," ('movie', 0.6605965),\n"," ('laugh', 0.5075881),\n"," ('dog', 0.4103828),\n"," ('cat', 0.34774196),\n"," ('fight', 0.2577684),\n"," ('building', 0.22762263)]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["test_words_similarity_map['comedy']"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1712524686150,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"SX8FSBGgbCWw","outputId":"c24f8e3e-a0ff-4f51-ed2c-d1e2d3ddea4b"},"outputs":[{"data":{"text/plain":["[('comedy', 0.5075881),\n"," ('movie', 0.50631833),\n"," ('fight', 0.36221814),\n"," ('action', 0.31770185),\n"," ('building', 0.31391907),\n"," ('dog', 0.30144316),\n"," ('plot', 0.29532665),\n"," ('cat', 0.17594783)]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["test_words_similarity_map['laugh']"]},{"cell_type":"markdown","metadata":{"id":"GIEFNgNHbZik"},"source":["## Finidng similar words\n","\n","Since the vectors now represent a dimension of similarity, word2vec also allows us to find similar words to a word (much like we did above but from the whole corpus instead of a selective words)\n","\n","word2vec also performs reasonably well for doing things like:\n","\n","king_vec - man_vec + woman_vec = queen_vec (a famous example). However for good results, do note that the model needs to be trained on siginificant data. Our model is trained on a very small data (relatively) to do things like this"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1712525140218,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"v0uPKOdkbuWm","outputId":"3f381a30-9f5d-4a60-f300-7e0755297bce"},"outputs":[{"data":{"text/plain":["[('film', 0.9537474513053894),\n"," ('flick', 0.8437069058418274),\n"," ('show', 0.8287235498428345),\n"," ('sequel', 0.774604320526123),\n"," ('thing', 0.7666338682174683),\n"," ('series', 0.7632709741592407),\n"," ('mess', 0.7579289078712463),\n"," ('crap', 0.7566468715667725),\n"," ('picture', 0.7493321895599365),\n"," ('case', 0.7326716184616089)]"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["word_vectors.most_similar('movie')"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1712525169993,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"nnhXCwmachtk","outputId":"c35a2ab5-acdb-4691-dec5-e1f0a29f97d4"},"outputs":[{"data":{"text/plain":["[('daughter', 0.9675498604774475),\n"," ('father', 0.9666376113891602),\n"," ('son', 0.950274646282196),\n"," ('wife', 0.9387264847755432),\n"," ('husband', 0.9386721253395081),\n"," ('boyfriend', 0.9364470839500427),\n"," ('sister', 0.9297873377799988),\n"," ('brother', 0.9262775778770447),\n"," ('dad', 0.9232807159423828),\n"," ('mom', 0.9198517203330994)]"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["word_vectors.most_similar('mother')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz8L7hohdC_r"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOvjuYqCmGIFU6bZw8Cc+PX","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
