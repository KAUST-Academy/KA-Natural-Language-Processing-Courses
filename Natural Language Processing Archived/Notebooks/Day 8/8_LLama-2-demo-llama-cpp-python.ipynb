{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mUswXp0-tlNV"},"outputs":[],"source":["from IPython.display import clear_output, Markdown, display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMHK_Wpytpr3"},"outputs":[],"source":["# No need to run this on colab. These libraries come pre-installed on colab\n","# %pip install torch torchvision torchaudio"]},{"cell_type":"markdown","metadata":{"id":"16xxZX0Ft4XL"},"source":["# Content:\n","\n","In this demo, we will take a look at the llama-v2 language model.\n","\n","To use llama-v2, we will use the llama-cpp-python library, which is a python converted version of the llama-cpp library\n","\n","For this, we need to install the library and to download the model weights file. The file can be downloaded from huggingface [repo](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) of [TheBloke](https://huggingface.co/TheBloke). Credits to him for quantizing the model, saving it in different formats like GGML and GGUF and sharing with the community. He has a lot of other models on his channel that you can check out, including different versions of llama (like the 70B param sized one and code llama etc)"]},{"cell_type":"markdown","metadata":{"id":"2mnqKpa4vvrk"},"source":["## Downloading model file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UejyX_MntcGt"},"outputs":[],"source":["!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"wvuFeDu3wZDd"},"source":["## Installing llama-cpp-python\n","\n","installing supports different versions of hardware acceleration.\n","\n","We will go with Cuda. Checkout the [Github Repo](https://github.com/abetlen/llama-cpp-python) for more options"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317219,"status":"ok","timestamp":1712190421119,"user":{"displayName":"Telha Bin Bilal","userId":"14169008838171441457"},"user_tz":-300},"id":"qG7Mz0UuwCty","outputId":"edd30f04-03de-4dfa-cacf-585c4c4ae62b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.59.tar.gz (37.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.10.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.59-cp310-cp310-manylinux_2_35_x86_64.whl size=38831179 sha256=1100ff5c2ff4803156a8073d02da4c102667cdd57aa36b92c59ebfaa2b17dd98\n","  Stored in directory: /root/.cache/pip/wheels/67/d1/01/78c9a09d3b1f348adbc9dad22b238dba3e20c12479986b8efa\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.59\n"]}],"source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python==0.2.74  # This takes a few mins when building wheel. Be patient."]},{"cell_type":"markdown","metadata":{"id":"k0F2JhjiyiiD"},"source":["## Running Llama-v2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caVzdA9vxri4"},"outputs":[],"source":["import json\n","\n","from llama_cpp import Llama"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTiK6FuFyqk6"},"outputs":[],"source":["model = Llama(\n","    \"llama-2-7b-chat.Q5_K_M.gguf\",\n","    n_gpu_layers=-1, # To use GPU\n","    n_ctx=2048, # default ctx window is 512\n","\n","    # llama-cpp supports multiple variations of llama and they can have different prompt format.\n","    # specifying the chat format isn't necessary but will let us use the create_chat_completions function.\n","    # alternatively we can just restructure our prompt and use the simple __call__\n","    chat_format=\"llama-2\",\n",")\n","\n","# clear_output()"]},{"cell_type":"markdown","metadata":{"id":"j60Xyt0b49KW"},"source":["Let's try a code generation example.\n","\n","Note that llama-v2 and llama-v2 chat isn't specifically trained for code generations but it has some capability for it\n","\n","Models trained for code generation are what's used by applications like github copilot or codenium.\n","\n","Llama-2, being a text generation model can be trained and used for that purpose as well.\n","There is even a version of llama-2 called codellama, build for code generation.\n","\n","The model file for codellama can also be found at TheBloke's repo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZM1D8ZG1ql8a"},"outputs":[],"source":["sys_msg = \"You are a Python coding instructor. Help your students with their code.\"\n","user_prompt = \"\"\"\n","Write me a function to calculate the fibbonaci series uptil length N.\n","Do NOT use recursion. Instead, use dynamic programming.\"\"\"\n","\n","prompt = f\"\"\"\n","[INST] <<SYS>>\n","{sys_msg}\n","<</SYS>>\n","{user_prompt}[/INST]\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13210,"status":"ok","timestamp":1712180580978,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"KCUObzRh1wtc","outputId":"c31bcf2e-55ab-497a-a0d4-e6838ef7fb17"},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     243.03 ms\n","llama_print_timings:      sample time =     248.98 ms /   400 runs   (    0.62 ms per token,  1606.55 tokens per second)\n","llama_print_timings: prompt eval time =     219.73 ms /    35 tokens (    6.28 ms per token,   159.28 tokens per second)\n","llama_print_timings:        eval time =   11043.40 ms /   399 runs   (   27.68 ms per token,    36.13 tokens per second)\n","llama_print_timings:       total time =   12900.07 ms /   434 tokens\n"]}],"source":["output = model(\n","    prompt,\n","    max_tokens=None  # sets no length limit\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1712180610536,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"ty1NLAjk10pr","outputId":"222f44d5-affc-4c87-cea4-a86e7dac8d91"},"outputs":[{"data":{"text/plain":["{'id': 'cmpl-3eea1755-e55c-42b5-b988-d020dd5e3355',\n"," 'object': 'text_completion',\n"," 'created': 1712180566,\n"," 'model': 'llama-2-7b-chat.Q5_K_M.gguf',\n"," 'choices': [{'text': \"Of course! Here is an example of how you could write a Python function to calculate the Fibonacci sequence up to a given length `N` using dynamic programming:\\n```\\ndef fibonacci(n):\\n    # Initialize a list to store the Fibonacci numbers\\n    fibs = [0, 1]\\n    # Loop until the length of the list is equal to N\\n    for _ in range(n-2):\\n        # Add the previous two Fibonacci numbers to the current one\\n        fibs.append(fibs[-1] + fibs[-2])\\n    return fibs\\n```\\nExplanation:\\n\\nThe idea behind this function is to use dynamic programming to compute the `n`-th Fibonacci number, `Fib(n)`. We do this by storing the first `n` Fibonacci numbers in a list, `fibs`, and then computing each subsequent number as a sum of the previous two.\\nThe function initializes `fibs` with the first two Fibonacci numbers, `0` and `1`. Then, it loops `n-2` times to compute the remaining `n`-th Fibonacci number. On each iteration, it adds the previous two Fibonacci numbers to the current one, which gives us the next number in the sequence.\\nHere's an example of how you could use this function:\\n```\\n# Calculate the Fibonacci sequence up to length 5\\nprint(fibonacci(5)) # [0, 1, 1, 2, 3]\\n```\\nThis will output the first five Fibonacci numbers: `0`, `1`, `1`, `2`, and `3`.\\nI hope this helps! Let me know if you have any questions or need further clarification.\",\n","   'index': 0,\n","   'logprobs': None,\n","   'finish_reason': 'stop'}],\n"," 'usage': {'prompt_tokens': 68, 'completion_tokens': 399, 'total_tokens': 467}}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1712180612628,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"YwZ68PIA2HHr","outputId":"2d9ae434-54e8-4b5d-8973-7b9aa1a1d50a"},"outputs":[{"data":{"text/plain":["{'text': \"Of course! Here is an example of how you could write a Python function to calculate the Fibonacci sequence up to a given length `N` using dynamic programming:\\n```\\ndef fibonacci(n):\\n    # Initialize a list to store the Fibonacci numbers\\n    fibs = [0, 1]\\n    # Loop until the length of the list is equal to N\\n    for _ in range(n-2):\\n        # Add the previous two Fibonacci numbers to the current one\\n        fibs.append(fibs[-1] + fibs[-2])\\n    return fibs\\n```\\nExplanation:\\n\\nThe idea behind this function is to use dynamic programming to compute the `n`-th Fibonacci number, `Fib(n)`. We do this by storing the first `n` Fibonacci numbers in a list, `fibs`, and then computing each subsequent number as a sum of the previous two.\\nThe function initializes `fibs` with the first two Fibonacci numbers, `0` and `1`. Then, it loops `n-2` times to compute the remaining `n`-th Fibonacci number. On each iteration, it adds the previous two Fibonacci numbers to the current one, which gives us the next number in the sequence.\\nHere's an example of how you could use this function:\\n```\\n# Calculate the Fibonacci sequence up to length 5\\nprint(fibonacci(5)) # [0, 1, 1, 2, 3]\\n```\\nThis will output the first five Fibonacci numbers: `0`, `1`, `1`, `2`, and `3`.\\nI hope this helps! Let me know if you have any questions or need further clarification.\",\n"," 'index': 0,\n"," 'logprobs': None,\n"," 'finish_reason': 'stop'}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["output[\"choices\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1712180614028,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"6SstiIZl14K4","outputId":"c9accd0b-d9bd-4f96-a30e-0d04ca74d2e6"},"outputs":[{"data":{"text/markdown":["Of course! Here is an example of how you could write a Python function to calculate the Fibonacci sequence up to a given length `N` using dynamic programming:\n","```\n","def fibonacci(n):\n","    # Initialize a list to store the Fibonacci numbers\n","    fibs = [0, 1]\n","    # Loop until the length of the list is equal to N\n","    for _ in range(n-2):\n","        # Add the previous two Fibonacci numbers to the current one\n","        fibs.append(fibs[-1] + fibs[-2])\n","    return fibs\n","```\n","Explanation:\n","\n","The idea behind this function is to use dynamic programming to compute the `n`-th Fibonacci number, `Fib(n)`. We do this by storing the first `n` Fibonacci numbers in a list, `fibs`, and then computing each subsequent number as a sum of the previous two.\n","The function initializes `fibs` with the first two Fibonacci numbers, `0` and `1`. Then, it loops `n-2` times to compute the remaining `n`-th Fibonacci number. On each iteration, it adds the previous two Fibonacci numbers to the current one, which gives us the next number in the sequence.\n","Here's an example of how you could use this function:\n","```\n","# Calculate the Fibonacci sequence up to length 5\n","print(fibonacci(5)) # [0, 1, 1, 2, 3]\n","```\n","This will output the first five Fibonacci numbers: `0`, `1`, `1`, `2`, and `3`.\n","I hope this helps! Let me know if you have any questions or need further clarification."],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["# response\n","display(Markdown(output[\"choices\"][0][\"text\"]))"]},{"cell_type":"markdown","metadata":{"id":"c-tQxdpO69wo"},"source":["Let's try a translation example now\n","\n","\n","\n","Like before, llama-v2 has some capability of translating but is not SPECIFICALLY trained for it\n","\n","The translations here will vary in quality depending on what languages are being used.\n","\n","The more of that language llama-v2 has seen during training, the better the results\n","\n","For example, the translation of english to japanese will be better than from english to arabic.\n","\n","instead of using llama-2-chat, it'd be better to use a finetuned seq2seq model for translations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rda-Lcfa6z2H"},"outputs":[],"source":["# Let's use create_chat_completions for now.\n","# Note that we need to specify the chat format for this to work correctly\n","\n","\n","sys_msg = \"You are state of the are languge translator who is assisting people with translations\"\n","user_prompt = \"\"\"\n","Translate this English to Japanese:\n","'I woke up really early in the morning and then went for a jog to freshen up my mind'\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6233,"status":"ok","timestamp":1712184356737,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"FipmD9rb74j_","outputId":"30bc8503-b507-4e45-deee-ce0c61760df3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     243.03 ms\n","llama_print_timings:      sample time =     134.06 ms /   202 runs   (    0.66 ms per token,  1506.83 tokens per second)\n","llama_print_timings: prompt eval time =     239.54 ms /    63 tokens (    3.80 ms per token,   263.01 tokens per second)\n","llama_print_timings:        eval time =    5265.27 ms /   201 runs   (   26.20 ms per token,    38.17 tokens per second)\n","llama_print_timings:       total time =    6422.72 ms /   264 tokens\n"]}],"source":["output = model.create_chat_completion(\n","    messages = [\n","          {\"role\": \"system\", \"content\": sys_msg},\n","          {\n","              \"role\": \"user\",\n","              \"content\": user_prompt\n","          }\n","      ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1712184358308,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Mi_KBqJI8PSj","outputId":"016bf668-3424-4172-9670-fa47f605bf89"},"outputs":[{"data":{"text/plain":["{'id': 'chatcmpl-85eeeeb4-9001-41bf-9e88-b3ae70787ce1',\n"," 'object': 'chat.completion',\n"," 'created': 1712184349,\n"," 'model': 'llama-2-7b-chat.Q5_K_M.gguf',\n"," 'choices': [{'index': 0,\n","   'message': {'role': 'assistant',\n","    'content': '  Sure, I\\'d be happy to help! Here\\'s the translation of \"I woke up really early in the morning and then went for a jog to freshen up my mind\" into Japanese:\\n「朝のうちに寝ていたが、脳を清めるためにランニングをした」\\nHere\\'s a breakdown of the translation:\\n* 朝 (asobu) - morning\\n* うちに (uchi ni) - in the morning\\n* 寝ていた (nite ita) - woke up\\n* 脳を清める (kokoro o kyūmu) - to freshen up my mind\\n* た (ta) - to\\n* ランニング (ranningu) - jogging\\nI hope this helps! Let me know if you have any other questions.'},\n","   'logprobs': None,\n","   'finish_reason': 'stop'}],\n"," 'usage': {'prompt_tokens': 73, 'completion_tokens': 201, 'total_tokens': 274}}"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1712184359911,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"vC5iU1uv8UNG","outputId":"23e88f77-7630-456d-88f3-c08d84dfed1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Sure, I'd be happy to help! Here's the translation of \"I woke up really early in the morning and then went for a jog to freshen up my mind\" into Japanese:\n","「朝のうちに寝ていたが、脳を清めるためにランニングをした」\n","Here's a breakdown of the translation:\n","* 朝 (asobu) - morning\n","* うちに (uchi ni) - in the morning\n","* 寝ていた (nite ita) - woke up\n","* 脳を清める (kokoro o kyūmu) - to freshen up my mind\n","* た (ta) - to\n","* ランニング (ranningu) - jogging\n","I hope this helps! Let me know if you have any other questions.\n"]}],"source":["print(output['choices'][0]['message']['content'])"]},{"cell_type":"markdown","metadata":{"id":"H6whTF6sHMyn"},"source":["For the next example, let's ask it to solve a simple mathematical equation\n","\n","for this example, we will also restrict it to produce JSON output which is more practical when the output needs to be further processed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOSLfCLU88-x"},"outputs":[],"source":["sys_msg = \"\"\"\n","You will be given reviews and your job is to analyse them, find and extract the negative parts.\n","\n","1. Only output negative parts. If a review has no negative experiences in it, just output [].\n","2. One review can have multiple negative parts. find them all.\n","3. Do not alter parts of reviews or add something that isn't in the review.\n","4. Use the same keyword for the same issue across reviews. Do not use multiple keywords for the same issue.\n","5. Make sure the output is a valid JSON\n","\n","Use this JSON format and dont say ANYHING other than this JSON:\n","\n","[\n","    {\n","        \"keyword-for-the-problem\": \"Part of the review mentioning the problem\"\n","    }\n","]\n","\n","\"\"\"\n","\n","user_prompt = \"\"\"\n","Analyse this review: The food was decent, but nothing extraordinary. It felt a bit overpriced for what we got. Service was a bit slow, but the staff was polite.\n","\"\"\"\n","\n","prompt = f\"\"\"\n","[INST] <<SYS>>\n","{sys_msg}\n","<</SYS>>\n","{user_prompt}[/INST]\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":869,"status":"ok","timestamp":1712192961344,"user":{"displayName":"Telha Bin Bilal","userId":"14169008838171441457"},"user_tz":-300},"id":"CjEljkDSInW3","outputId":"664c68b3-9221-461c-e429-79877ee7c039"},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     553.30 ms\n","llama_print_timings:      sample time =      11.29 ms /    20 runs   (    0.56 ms per token,  1771.79 tokens per second)\n","llama_print_timings: prompt eval time =     348.96 ms /   226 tokens (    1.54 ms per token,   647.64 tokens per second)\n","llama_print_timings:        eval time =     478.23 ms /    19 runs   (   25.17 ms per token,    39.73 tokens per second)\n","llama_print_timings:       total time =     891.98 ms /   245 tokens\n"]}],"source":["output = model(\n","    prompt,\n","    max_tokens=None  # sets no length limit\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGdcWA70IrF2"},"outputs":[],"source":["output = json.loads(output[\"choices\"][0][\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712192965106,"user":{"displayName":"Telha Bin Bilal","userId":"14169008838171441457"},"user_tz":-300},"id":"yaABPAkmJ4YJ","outputId":"536b5653-0a3c-48b2-909a-3b4d7b7c6b89"},"outputs":[{"data":{"text/plain":["{'keyword-for-the-problem': 'overpriced'}"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["output"]},{"cell_type":"markdown","metadata":{"id":"Qm4WEyLnJarN"},"source":["As most likely will be the case when you re-run this, the output has some issues.\n","\n","Let's try **one shotting** by adding an example in the prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2094,"status":"ok","timestamp":1712193426963,"user":{"displayName":"Telha Bin Bilal","userId":"14169008838171441457"},"user_tz":-300},"id":"JAQUP5vnItCi","outputId":"44c25a5a-d2e4-4308-e2e6-7a52f273d911"},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     553.30 ms\n","llama_print_timings:      sample time =      25.35 ms /    50 runs   (    0.51 ms per token,  1972.08 tokens per second)\n","llama_print_timings: prompt eval time =     216.36 ms /    40 tokens (    5.41 ms per token,   184.88 tokens per second)\n","llama_print_timings:        eval time =    1256.43 ms /    49 runs   (   25.64 ms per token,    39.00 tokens per second)\n","llama_print_timings:       total time =    1619.58 ms /    89 tokens\n"]}],"source":["sys_msg = \"\"\"\n","You will be given reviews and your job is to analyse them, find and extract the negative parts.\n","\n","1. Only output negative parts. If a review has no negative experiences in it, just output [].\n","2. One review can have multiple negative parts. find them all.\n","3. Do not alter parts of reviews or add something that isn't in the review.\n","4. Use the same keyword for the same issue across reviews. Do not use multiple keywords for the same issue.\n","5. Make sure the output is a valid JSON\n","\n","Use this JSON format and dont say ANYHING other than this JSON:\n","\n","[\n","    {\n","        \"keyword-for-the-problem\": \"Part of the review mentioning the problem\"\n","    }\n","]\n","\n","Here is an example:\n","\n","Review: `Decent food, but nothing outstanding. The service was average, and the atmosphere was a bit lacking. It's an okay option if you're in the area.`\n","\n","[\n","  {\n","    \"boring-ambience\": \"atmosphere was a bit lacking\"\n","  }\n","]\n","\n","\"\"\"\n","\n","user_prompt = \"\"\"\n","\n","Review: `The food was decent, but nothing extraordinary. It felt a bit overpriced for what we got. Service was a bit slow, but the staff was polite.`\n","\"\"\"\n","\n","prompt = f\"\"\"\n","[INST]<<SYS>>\n","{sys_msg}\n","<</SYS>>\n","{user_prompt}[/INST]\n","\"\"\"\n","\n","output = model(\n","    prompt,\n","    max_tokens=None,  # sets no length limit\n","    stop=['<END>']\n",")\n","\n","output = json.loads(output[\"choices\"][0][\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1712193428839,"user":{"displayName":"Telha Bin Bilal","userId":"14169008838171441457"},"user_tz":-300},"id":"lpWu516NrNtl","outputId":"268462df-59a4-45fe-b047-a8e4c82d8f2c"},"outputs":[{"data":{"text/plain":["[{'overpriced': 'It felt a bit overpriced for what we got'},\n"," {'slow-service': 'Service was a bit slow'}]"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKpEaQHHMN5B"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
