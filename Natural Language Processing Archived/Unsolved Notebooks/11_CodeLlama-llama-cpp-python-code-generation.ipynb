{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":848,"status":"ok","timestamp":1719692791836,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"mUswXp0-tlNV"},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":680,"status":"ok","timestamp":1719692797584,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"DMHK_Wpytpr3"},"outputs":[],"source":["# No need to run this on colab. These libraries come pre-installed on colab\n","# %pip install torch torchvision torchaudio"]},{"cell_type":"markdown","metadata":{"id":"16xxZX0Ft4XL"},"source":["# Content:\n","\n","In this demo, we will take do some AI-based code generation, like the kind done my github co-pilot or codenium or other code completion services.\n","\n","The model we will use is codeLlama. CodeLlama models are basically llama-v2 models fine tuned for coding tasks. Code llama is available in different sizes and we'll use the 7B params model variant.\n","\n","\n","For this, we need to install the library and to download the model weights file. The file can be downloaded from huggingface [repo](https://huggingface.co/TheBloke/CodeLlama-7B-GGUF) of [TheBloke](https://huggingface.co/TheBloke). Credits to him for quantizing the model, saving it in different formats like GGML and GGUF and sharing with the community. He has a lot of other models on his channel that you can check out, including different versions of llama"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
