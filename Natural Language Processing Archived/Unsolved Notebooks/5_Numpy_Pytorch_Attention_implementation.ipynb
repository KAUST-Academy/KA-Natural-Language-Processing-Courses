{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":748,"status":"ok","timestamp":1711125366922,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"sjGV6cBoJRGk"},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1711125367654,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"ww2advIuJU6f"},"outputs":[],"source":["# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n","\n","# %pip install torch\n","# %pip install numpy\n","# %pip install matplotlib\n","\n","# clear_output()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3362,"status":"ok","timestamp":1711125371015,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"T1N4Y9jNJZ3D"},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"wqJg33HoJeL8"},"source":["# Contents\n","\n","In this notebook, you are required implement attention by hand in numpy and pytorch"]},{"cell_type":"markdown","metadata":{"id":"ZBqPsxS1KuWI"},"source":["## Numpy Implementation\n","\n","No need to make a trainable version of attention(you don't need to write the code for backpropogation in numpy implementation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":531},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1711128361514,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Wui5Z8xrPFDU","outputId":"6d6ead3f-875f-4d95-9e6d-9009c9eb1195"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9MYvEm5HSKCF"},"source":["## Pytorch Implementation\n","\n","In this implementation, we want trainable attention. Make sure you also calculate and return attention weights in addition to attended values in this implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmhLV1d1N4OA"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO7h4q0YlymvFkA50wVrSQS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
