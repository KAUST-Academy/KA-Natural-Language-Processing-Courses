{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2OoUWgrFi82h"},"outputs":[],"source":["from IPython.display import clear_output, display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN1Z7uGbxTdq","scrolled":true},"outputs":[],"source":["# %pip install torch torchvision pillow spacy numpy\n","# %pip install torchtext\n","# %pip install pycocotools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EDyhvMIxybA"},"outputs":[],"source":["import os\n","import math\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import CocoCaptions\n","from torchtext.data.utils import get_tokenizer\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","\n","from PIL import Image\n","import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4aD7hcPxTdr"},"outputs":[],"source":["dataset_variant = 'val2017'"]},{"cell_type":"markdown","metadata":{},"source":["# Contents\n","\n","- In this notebook, you are required to make a Transformer(in decoder) based image captioning model using PyTorch. Use a custom transformer class for your decoder and not a pre-defined model architecture. You can use nn.TransformerDecoder and nn.TransformerDecoderLayer if you want.\n","- Use the Coco 2017 dataset. You dont need to use the entire train variant as it can get too big.\n","- After training, show your model's performance by taking a few images and using the model to generate their captions and then visualizing the images and their captions\n","- For image feature extraction it's recommended to use a pre-trained model's features(not final output but intermediate layer outputs) as a backbone model(which will be our image encoder) and then feed them to the Transformer decoder. You can set requires_grad of the backbone model to False as we don't need to train it and it'll improve the training speed significantly. A good example of a backbone model is ResNet"]},{"cell_type":"markdown","metadata":{"id":"wijrNMlqxi5R"},"source":["## Downloading the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlPorEo6jseG"},"outputs":[],"source":["# Define paths for dataset and annotations\n","data_dir = './data'\n","images_dir = os.path.join(data_dir, dataset_variant)\n","annotations_dir = os.path.join(data_dir, 'annotations')\n","\n","# Create directories if they don't exist\n","if not os.path.exists(data_dir):\n","    os.makedirs(data_dir)\n","if not os.path.exists(images_dir):\n","    os.makedirs(images_dir)\n","if not os.path.exists(annotations_dir):\n","    os.makedirs(annotations_dir)\n","\n","# Download dataset\n","!wget http://images.cocodataset.org/zips/{dataset_variant}.zip -P {data_dir}\n","\n","# Unzip dataset\n","!unzip {data_dir}/{dataset_variant}.zip -d {data_dir}\n","\n","clear_output()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQeFlwUfC-U0"},"outputs":[],"source":["# Download annotations\n","!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P {annotations_dir}\n","\n","# # Unzip annotations\n","!unzip {annotations_dir}/annotations_trainval2017.zip -d {annotations_dir}\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"uYIiLwcix_8n"},"source":["## Loading the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1712262590772,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"JwP-q5wjmo4b","outputId":"8b1fdb90-63cb-45a4-a733-05c223153e33"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.04s)\n","creating index...\n","index created!\n"]}],"source":["transform = transforms.Compose(\n","        [\n","            transforms.Resize((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","        ]\n","    )\n","\n","# Load MS-COCO dataset\n","train_dataset = CocoCaptions(root=f'./data/{dataset_variant}', annFile=f'./data/annotations/annotations/captions_{dataset_variant}.json', transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You might need to create a colate function(or another custom dataset class which works on top of the CocoCaptions class) for handling certain\n","# processing of your dataset like handling the captions different length scenario or converting text tokens to numeric representation etc.v"]},{"cell_type":"markdown","metadata":{},"source":["## Building the tokenizer and vocabulary\n","\n","It's recommended to create a tokenizer and a vocabulary for your text corpus but you can solve it your own way if you want.\n","\n","A recommendation for text tokenizer is Spacy but it's not mandatory. You can use another library or write your own custom tokenizer if you want"]},{"cell_type":"markdown","metadata":{"id":"i79_gIk-1zut"},"source":["## Defining the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D59I4JSZe46D"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oOQLLt_F3iIs"},"source":["## Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpeCGaogiE0Q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing the results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
