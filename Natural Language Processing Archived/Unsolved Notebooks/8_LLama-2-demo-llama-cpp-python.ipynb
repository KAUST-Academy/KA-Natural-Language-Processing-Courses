{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mUswXp0-tlNV"},"outputs":[],"source":["from IPython.display import clear_output, Markdown, display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMHK_Wpytpr3"},"outputs":[],"source":["# No need to run this on colab. These libraries come pre-installed on colab\n","# %pip install torch torchvision torchaudio"]},{"cell_type":"markdown","metadata":{"id":"16xxZX0Ft4XL"},"source":["# Content:\n","\n","In this demo, we will take a look at the llama-v2 language model.\n","\n","To use llama-v2, we will use the llama-cpp-python library, which is a python converted version of the llama-cpp library\n","\n","For this, we need to install the library and to download the model weights file. The file can be downloaded from huggingface [repo](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF) of [TheBloke](https://huggingface.co/TheBloke). Credits to him for quantizing the model, saving it in different formats like GGML and GGUF and sharing with the community. He has a lot of other models on his channel that you can check out, including different versions of llama (like the 70B param sized one and code llama etc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKpEaQHHMN5B"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
